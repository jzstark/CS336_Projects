<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <title id="head-title">report.html</title>
      <link href="assets\style.css" rel="stylesheet" type="text/css"/>
  </head>
  <body>
    <h1 id="title">report.html</h1>
    <p>Report generated on 09-Jul-2025 at 13:44:27 by <a href="https://pypi.python.org/pypi/pytest-html">pytest-html</a>
        v4.1.1</p>
    <div id="environment-header">
      <h2>Environment</h2>
    </div>
    <table id="environment"></table>
    <!-- TEMPLATES -->
      <template id="template_environment_row">
      <tr>
        <td></td>
        <td></td>
      </tr>
    </template>
    <template id="template_results-table__body--empty">
      <tbody class="results-table-row">
        <tr id="not-found-message">
          <td colspan="4">No results found. Check the filters.</th>
        </tr>
    </template>
    <template id="template_results-table__tbody">
      <tbody class="results-table-row">
        <tr class="collapsible">
        </tr>
        <tr class="extras-row">
          <td class="extra" colspan="4">
            <div class="extraHTML"></div>
            <div class="media">
              <div class="media-container">
                  <div class="media-container__nav--left"><</div>
                  <div class="media-container__viewport">
                    <img src="" />
                    <video controls>
                      <source src="" type="video/mp4">
                    </video>
                  </div>
                  <div class="media-container__nav--right">></div>
                </div>
                <div class="media__name"></div>
                <div class="media__counter"></div>
            </div>
            <div class="logwrapper">
              <div class="logexpander"></div>
              <div class="log"></div>
            </div>
          </td>
        </tr>
      </tbody>
    </template>
    <!-- END TEMPLATES -->
    <div class="summary">
      <div class="summary__data">
        <h2>Summary</h2>
        <div class="additional-summary prefix">
        </div>
        <p class="run-count">46 tests took 00:00:23.</p>
        <p class="filter">(Un)check the boxes to filter the results.</p>
        <div class="summary__reload">
          <div class="summary__reload__button hidden" onclick="location.reload()">
            <div>There are still tests running. <br />Reload this page to get the latest results!</div>
          </div>
        </div>
        <div class="summary__spacer"></div>
        <div class="controls">
          <div class="filters">
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="failed" />
            <span class="failed">46 Failed,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="passed" disabled/>
            <span class="passed">0 Passed,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="skipped" />
            <span class="skipped">2 Skipped,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="xfailed" disabled/>
            <span class="xfailed">0 Expected failures,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="xpassed" disabled/>
            <span class="xpassed">0 Unexpected passes,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="error" disabled/>
            <span class="error">0 Errors,</span>
            <input checked="true" class="filter" name="filter_checkbox" type="checkbox" data-test-result="rerun" disabled/>
            <span class="rerun">0 Reruns</span>
          </div>
          <div class="collapse">
            <button id="show_all_details">Show all details</button>&nbsp;/&nbsp;<button id="hide_all_details">Hide all details</button>
          </div>
        </div>
      </div>
      <div class="additional-summary summary">
      </div>
      <div class="additional-summary postfix">
      </div>
    </div>
    <table id="results-table">
      <thead id="results-table-head">
        <tr>
          <th class="sortable" data-column-type="result">Result</th>
          <th class="sortable" data-column-type="testId">Test</th>
          <th class="sortable" data-column-type="duration">Duration</th>
          <th>Links</th>
        </tr>
      </thead>
    </table>
  </body>
  <footer>
    <div id="data-container" data-jsonblob="{&#34;environment&#34;: {&#34;Python&#34;: &#34;3.12.4&#34;, &#34;Platform&#34;: &#34;Windows-11-10.0.26100-SP0&#34;, &#34;Packages&#34;: {&#34;pytest&#34;: &#34;8.4.1&#34;, &#34;pluggy&#34;: &#34;1.6.0&#34;}, &#34;Plugins&#34;: {&#34;anyio&#34;: &#34;4.2.0&#34;, &#34;jaxtyping&#34;: &#34;0.3.2&#34;, &#34;html&#34;: &#34;4.1.1&#34;, &#34;metadata&#34;: &#34;3.1.1&#34;}}, &#34;tests&#34;: {&#34;tests/test_data.py::test_get_batch&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_data.py::test_get_batch&#34;, &#34;duration&#34;: &#34;1 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_data.py::test_get_batch&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;1 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_get_batch():\n        dataset = np.arange(0, 100)\n        context_length = 7\n        batch_size = 32\n        device = &amp;quot;cpu&amp;quot;\n    \n        # Sanity check to make sure that the random samples are indeed somewhat random.\n        starting_indices = Counter()\n        num_iters = 1000\n        for _ in range(num_iters):\n&amp;gt;           x, y = run_get_batch(\n                dataset=dataset,\n                batch_size=batch_size,\n                context_length=context_length,\n                device=device,\n            )\n\ntests\\test_data.py:20: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ndataset = array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, ...72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\nbatch_size = 32, context_length = 7, device = &amp;#x27;cpu&amp;#x27;\n\n    def run_get_batch(\n        dataset: npt.NDArray, batch_size: int, context_length: int, device: str\n    ) -&amp;gt; tuple[torch.Tensor, torch.Tensor]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Given a dataset (a 1D numpy array of integers) and a desired batch size and\n        context length, sample language modeling input sequences and their corresponding\n        labels from the dataset.\n    \n        Args:\n            dataset (np.array): 1D numpy array of integer token IDs in the dataset.\n            batch_size (int): Desired batch size to sample.\n            context_length (int): Desired context length of each sampled example.\n            device (str): PyTorch device string (e.g., &amp;#x27;cpu&amp;#x27; or &amp;#x27;cuda:0&amp;#x27;) indicating the device\n                to place the sampled input sequences and labels on.\n    \n        Returns:\n            Tuple of torch.LongTensors of shape (batch_size, context_length). The first tuple item\n            is the sampled input sequences, and the second tuple item is the corresponding\n            language modeling labels.\n        &amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       raise NotImplementedError\nE       NotImplementedError\n\ntests\\adapters.py:420: NotImplementedError\n&#34;}], &#34;tests/test_model.py::test_linear&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_model.py::test_linear&#34;, &#34;duration&#34;: &#34;36 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_model.py::test_linear&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;36 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;numpy_snapshot = &amp;lt;tests.conftest.NumpySnapshot object at 0x0000021B0B0485F0&amp;gt;\nts_state_dict = ({&amp;#x27;layers.0.attn.k_proj.weight&amp;#x27;: tensor([[ 0.0890,  0.1049,  0.0980,  ...,  0.1314,  0.0594, -0.0206],\n        [ 0.027...7,  ...,  0.0380, -0.0240,  0.1170]]), ...}, {&amp;#x27;context_length&amp;#x27;: 16, &amp;#x27;d_ff&amp;#x27;: 128, &amp;#x27;d_model&amp;#x27;: 64, &amp;#x27;ffn_type&amp;#x27;: None, ...})\nin_embeddings = tensor([[[-0.9414,  1.2632, -0.1838,  ..., -0.1941,  0.0048, -1.3165],\n         [ 0.0204, -0.1652,  0.2109,  ...,  0.6...73, -0.5486,  ...,  1.5676, -1.1472, -1.5822],\n         [ 1.5936, -0.4113,  0.6037,  ..., -0.6494,  1.2858,  0.4321]]])\nd_model = 64, d_ff = 128\n\n    def test_linear(numpy_snapshot, ts_state_dict, in_embeddings, d_model, d_ff):\n        w1_weight = ts_state_dict[0][f&amp;quot;layers.0.ffn.w1.weight&amp;quot;]\n&amp;gt;       output = run_linear(\n            d_in=d_model,\n            d_out=d_ff,\n            weights=w1_weight,\n            in_features=in_embeddings,\n        )\n\ntests\\test_model.py:23: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nd_in = 64, d_out = 128\nweights = tensor([[ 0.0547,  0.0756,  0.0423,  ...,  0.1840,  0.0190, -0.0732],\n        [-0.0683,  0.0482,  0.0472,  ...,  0.193...1942, -0.0789,  ...,  0.0045, -0.1143,  0.0156],\n        [-0.1401, -0.1757, -0.0704,  ..., -0.0199, -0.0032, -0.0454]])\nin_features = tensor([[[-0.9414,  1.2632, -0.1838,  ..., -0.1941,  0.0048, -1.3165],\n         [ 0.0204, -0.1652,  0.2109,  ...,  0.6...73, -0.5486,  ...,  1.5676, -1.1472, -1.5822],\n         [ 1.5936, -0.4113,  0.6037,  ..., -0.6494,  1.2858,  0.4321]]])\n\n    def run_linear(\n        d_in: int,\n        d_out: int,\n        weights: Float[Tensor, &amp;quot; d_out d_in&amp;quot;],\n        in_features: Float[Tensor, &amp;quot; ... d_in&amp;quot;],\n    ) -&amp;gt; Float[Tensor, &amp;quot; ... d_out&amp;quot;]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Given the weights of a Linear layer, compute the transformation of a batched input.\n    \n        Args:\n            in_dim (int): The size of the input dimension\n            out_dim (int): The size of the output dimension\n            weights (Float[Tensor, &amp;quot;d_out d_in&amp;quot;]): The linear weights to use\n            in_features (Float[Tensor, &amp;quot;... d_in&amp;quot;]): The output tensor to apply the function to\n    \n        Returns:\n            Float[Tensor, &amp;quot;... d_out&amp;quot;]: The transformed output of your linear module.\n        &amp;quot;&amp;quot;&amp;quot;\n    \n&amp;gt;       raise NotImplementedError\nE       NotImplementedError\n\ntests\\adapters.py:34: NotImplementedError\n&#34;}], &#34;tests/test_model.py::test_embedding&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_model.py::test_embedding&#34;, &#34;duration&#34;: &#34;6 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_model.py::test_embedding&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;6 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;numpy_snapshot = &amp;lt;tests.conftest.NumpySnapshot object at 0x0000021B0B04B770&amp;gt;\nts_state_dict = ({&amp;#x27;layers.0.attn.k_proj.weight&amp;#x27;: tensor([[ 0.0890,  0.1049,  0.0980,  ...,  0.1314,  0.0594, -0.0206],\n        [ 0.027...7,  ...,  0.0380, -0.0240,  0.1170]]), ...}, {&amp;#x27;context_length&amp;#x27;: 16, &amp;#x27;d_ff&amp;#x27;: 128, &amp;#x27;d_model&amp;#x27;: 64, &amp;#x27;ffn_type&amp;#x27;: None, ...})\nin_indices = tensor([[5130, 8761, 2403, 3492, 2234, 1437, 5647, 7786, 1904, 6558, 9177, 7649],\n        [8235, 4285, 4217, 5482, 574... 861, 9322, 4143, 1833, 4502, 2050],\n        [ 907, 5061, 9813, 7793, 6212, 7893, 3454, 8112, 6322, 2511, 2434, 6879]])\nvocab_size = 10000, d_model = 64\n\n    def test_embedding(numpy_snapshot, ts_state_dict, in_indices, vocab_size, d_model):\n        embedding_weight = ts_state_dict[0][f&amp;quot;token_embeddings.weight&amp;quot;]\n&amp;gt;       output = run_embedding(\n            vocab_size=vocab_size,\n            d_model=d_model,\n            weights=embedding_weight,\n            token_ids=in_indices,\n        )\n\ntests\\test_model.py:36: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nvocab_size = 10000, d_model = 64\nweights = tensor([[ 0.6384, -0.3678,  0.2761,  ..., -0.1542, -0.6972,  0.4515],\n        [ 0.2662, -0.5991,  0.7674,  ...,  0.409...1477,  0.3167,  ..., -0.0552, -0.8565, -0.3290],\n        [-0.9148, -0.5046,  0.6417,  ...,  0.2892, -0.4009, -0.0566]])\ntoken_ids = tensor([[5130, 8761, 2403, 3492, 2234, 1437, 5647, 7786, 1904, 6558, 9177, 7649],\n        [8235, 4285, 4217, 5482, 574... 861, 9322, 4143, 1833, 4502, 2050],\n        [ 907, 5061, 9813, 7793, 6212, 7893, 3454, 8112, 6322, 2511, 2434, 6879]])\n\n    def run_embedding(\n        vocab_size: int,\n        d_model: int,\n        weights: Float[Tensor, &amp;quot; vocab_size d_model&amp;quot;],\n        token_ids: Int[Tensor, &amp;quot; ...&amp;quot;],\n    ) -&amp;gt; Float[Tensor, &amp;quot; ... d_model&amp;quot;]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Given the weights of an Embedding layer, get the embeddings for a batch of token ids.\n    \n        Args:\n            vocab_size (int): The number of embeddings in the vocabulary\n            d_model (int): The size of the embedding dimension\n            weights (Float[Tensor, &amp;quot;vocab_size d_model&amp;quot;]): The embedding vectors to fetch from\n            token_ids (Int[Tensor, &amp;quot;...&amp;quot;]): The set of token ids to fetch from the Embedding layer\n    \n        Returns:\n            Float[Tensor, &amp;quot;... d_model&amp;quot;]: Batch of embeddings returned by your Embedding layer.\n        &amp;quot;&amp;quot;&amp;quot;\n    \n&amp;gt;       raise NotImplementedError\nE       NotImplementedError\n\ntests\\adapters.py:56: NotImplementedError\n&#34;}], &#34;tests/test_model.py::test_swiglu&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_model.py::test_swiglu&#34;, &#34;duration&#34;: &#34;5 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_model.py::test_swiglu&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;5 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;numpy_snapshot = &amp;lt;tests.conftest.NumpySnapshot object at 0x0000021B0B245A60&amp;gt;\nts_state_dict = ({&amp;#x27;layers.0.attn.k_proj.weight&amp;#x27;: tensor([[ 0.0890,  0.1049,  0.0980,  ...,  0.1314,  0.0594, -0.0206],\n        [ 0.027...7,  ...,  0.0380, -0.0240,  0.1170]]), ...}, {&amp;#x27;context_length&amp;#x27;: 16, &amp;#x27;d_ff&amp;#x27;: 128, &amp;#x27;d_model&amp;#x27;: 64, &amp;#x27;ffn_type&amp;#x27;: None, ...})\nin_embeddings = tensor([[[-0.9414,  1.2632, -0.1838,  ..., -0.1941,  0.0048, -1.3165],\n         [ 0.0204, -0.1652,  0.2109,  ...,  0.6...73, -0.5486,  ...,  1.5676, -1.1472, -1.5822],\n         [ 1.5936, -0.4113,  0.6037,  ..., -0.6494,  1.2858,  0.4321]]])\nd_model = 64, d_ff = 128\n\n    def test_swiglu(numpy_snapshot, ts_state_dict, in_embeddings, d_model, d_ff):\n        # reference_weights = torch.load(FIXTURES_PATH / &amp;quot;positionwise_feedforward_weights.pt&amp;quot;)\n        w1_weight, w2_weight, w3_weight = [ts_state_dict[0][f&amp;quot;layers.0.ffn.{k}.weight&amp;quot;] for k in [&amp;quot;w1&amp;quot;, &amp;quot;w2&amp;quot;, &amp;quot;w3&amp;quot;]]\n    \n&amp;gt;       actual_output = run_swiglu(\n            d_model=d_model,\n            d_ff=d_ff,\n            w1_weight=w1_weight,\n            w2_weight=w2_weight,\n            w3_weight=w3_weight,\n            in_features=in_embeddings,\n        )\n\ntests\\test_model.py:51: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nd_model = 64, d_ff = 128\nw1_weight = tensor([[ 0.0547,  0.0756,  0.0423,  ...,  0.1840,  0.0190, -0.0732],\n        [-0.0683,  0.0482,  0.0472,  ...,  0.193...1942, -0.0789,  ...,  0.0045, -0.1143,  0.0156],\n        [-0.1401, -0.1757, -0.0704,  ..., -0.0199, -0.0032, -0.0454]])\nw2_weight = tensor([[ 1.8391e-01, -1.0857e-01, -6.5546e-02,  ..., -2.0131e-02,\n         -1.3820e-01, -1.5189e-01],\n        [ 8.824..., -5.9488e-02],\n        [ 4.7624e-02,  6.6320e-02,  2.5070e-02,  ..., -1.8528e-01,\n         -2.3557e-01,  1.5298e-04]])\nw3_weight = tensor([[-0.0112, -0.0014,  0.0783,  ..., -0.1642,  0.1339,  0.1096],\n        [ 0.0508,  0.1094,  0.0124,  ...,  0.012...1727, -0.0652,  ..., -0.0104,  0.2612,  0.1444],\n        [ 0.0774, -0.0382, -0.0424,  ...,  0.0895, -0.0262,  0.0613]])\nin_features = tensor([[[-0.9414,  1.2632, -0.1838,  ..., -0.1941,  0.0048, -1.3165],\n         [ 0.0204, -0.1652,  0.2109,  ...,  0.6...73, -0.5486,  ...,  1.5676, -1.1472, -1.5822],\n         [ 1.5936, -0.4113,  0.6037,  ..., -0.6494,  1.2858,  0.4321]]])\n\n    def run_swiglu(\n        d_model: int,\n        d_ff: int,\n        w1_weight: Float[Tensor, &amp;quot; d_ff d_model&amp;quot;],\n        w2_weight: Float[Tensor, &amp;quot; d_model d_ff&amp;quot;],\n        w3_weight: Float[Tensor, &amp;quot; d_ff d_model&amp;quot;],\n        in_features: Float[Tensor, &amp;quot; ... d_model&amp;quot;],\n    ) -&amp;gt; Float[Tensor, &amp;quot; ... d_model&amp;quot;]:\n        &amp;quot;&amp;quot;&amp;quot;Given the weights of a SwiGLU network, return\n        the output of your implementation with these weights.\n    \n        Args:\n            d_model (int): Dimensionality of the feedforward input and output.\n            d_ff (int): Dimensionality of the up-project happening internally to your swiglu.\n            w1_weight (Float[Tensor, &amp;quot;d_ff d_model&amp;quot;]): Stored weights for W1\n            w2_weight (Float[Tensor, &amp;quot;d_model d_ff&amp;quot;]): Stored weights for W2\n            w3_weight (Float[Tensor, &amp;quot;d_ff d_model&amp;quot;]): Stored weights for W3\n            in_features (Float[Tensor, &amp;quot;... d_model&amp;quot;]): Input embeddings to the feed-forward layer.\n    \n        Returns:\n            Float[Tensor, &amp;quot;... d_model&amp;quot;]: Output embeddings of the same shape as the input embeddings.\n        &amp;quot;&amp;quot;&amp;quot;\n        # Example:\n        # If your state dict keys match, you can use `load_state_dict()`\n        # swiglu.load_state_dict(weights)\n        # You can also manually assign the weights\n        # swiglu.w1.weight.data = w1_weight\n        # swiglu.w2.weight.data = w2_weight\n        # swiglu.w3.weight.data = w3_weight\n&amp;gt;       raise NotImplementedError\nE       NotImplementedError\n\ntests\\adapters.py:88: NotImplementedError\n&#34;}], &#34;tests/test_model.py::test_scaled_dot_product_attention&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_model.py::test_scaled_dot_product_attention&#34;, &#34;duration&#34;: &#34;3 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_model.py::test_scaled_dot_product_attention&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;3 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;numpy_snapshot = &amp;lt;tests.conftest.NumpySnapshot object at 0x0000021B0B246B70&amp;gt;\nq = tensor([[[-1.5256e+00, -7.5023e-01, -6.5398e-01,  ..., -8.6959e-01,\n          -3.3312e+00, -7.4787e-01],\n         [-2....3.2825e-01],\n         [-1.8315e-01,  2.0009e+00,  1.4760e-01,  ...,  1.2993e+00,\n          -5.7777e-01,  1.4148e+00]]])\nk = tensor([[[-1.0408,  0.9166, -1.3042,  ...,  0.7787, -0.7749, -0.1398],\n         [ 1.1414, -0.6354, -1.4702,  ...,  1.8...43,  0.2841,  ..., -0.6094,  0.1403,  1.3990],\n         [-0.1150,  0.0779,  1.3394,  ..., -0.3242, -0.8369,  0.8859]]])\nv = tensor([[[-0.0766,  0.3599, -0.7820,  ..., -0.5296,  1.3544,  1.3778],\n         [-0.0752, -0.4233,  0.4217,  ...,  2.3...59,  1.1937,  ...,  0.4603, -0.9189,  0.2698],\n         [-0.7093, -1.5744,  1.5026,  ..., -1.7090,  0.2552,  0.8367]]])\nmask = tensor([[[ True,  True, False, False,  True, False, False, False, False, False,\n          False, False,  True, False, ...,  True, False, False, False,  True, False,  True, False,  True,\n          False, False, False, False, False, False]]])\n\n    def test_scaled_dot_product_attention(numpy_snapshot, q, k, v, mask):\n        # torch.manual_seed(42)\n        # Take the first batch item, so we test the 3D case\n        # (input shape (batch_size, seq_len, d_k)) for scaled dot-product attention.\n        # K = torch.load(FIXTURES_PATH / &amp;quot;scaled_dot_product_attention_K.pt&amp;quot;)[0]\n        # Q = torch.load(FIXTURES_PATH / &amp;quot;scaled_dot_product_attention_Q.pt&amp;quot;)[0]\n        # V = torch.load(FIXTURES_PATH / &amp;quot;scaled_dot_product_attention_V.pt&amp;quot;)[0]\n        # mask = torch.load(FIXTURES_PATH / &amp;quot;scaled_dot_product_attention_mask.pt&amp;quot;)\n        # expected_output = torch.load(FIXTURES_PATH / &amp;quot;scaled_dot_product_attention_expected_output.pt&amp;quot;)[0]\n&amp;gt;       actual_output = run_scaled_dot_product_attention(Q=q, K=k, V=v, mask=mask)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_model.py:71: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nQ = tensor([[[-1.5256e+00, -7.5023e-01, -6.5398e-01,  ..., -8.6959e-01,\n          -3.3312e+00, -7.4787e-01],\n         [-2....3.2825e-01],\n         [-1.8315e-01,  2.0009e+00,  1.4760e-01,  ...,  1.2993e+00,\n          -5.7777e-01,  1.4148e+00]]])\nK = tensor([[[-1.0408,  0.9166, -1.3042,  ...,  0.7787, -0.7749, -0.1398],\n         [ 1.1414, -0.6354, -1.4702,  ...,  1.8...43,  0.2841,  ..., -0.6094,  0.1403,  1.3990],\n         [-0.1150,  0.0779,  1.3394,  ..., -0.3242, -0.8369,  0.8859]]])\nV = tensor([[[-0.0766,  0.3599, -0.7820,  ..., -0.5296,  1.3544,  1.3778],\n         [-0.0752, -0.4233,  0.4217,  ...,  2.3...59,  1.1937,  ...,  0.4603, -0.9189,  0.2698],\n         [-0.7093, -1.5744,  1.5026,  ..., -1.7090,  0.2552,  0.8367]]])\nmask = tensor([[[ True,  True, False, False,  True, False, False, False, False, False,\n          False, False,  True, False, ...,  True, False, False, False,  True, False,  True, False,  True,\n          False, False, False, False, False, False]]])\n\n    def run_scaled_dot_product_attention(\n        Q: Float[Tensor, &amp;quot; ... queries d_k&amp;quot;],\n        K: Float[Tensor, &amp;quot; ... keys d_k&amp;quot;],\n        V: Float[Tensor, &amp;quot; ... values d_v&amp;quot;],\n        mask: Float[Tensor, &amp;quot; ... queries keys&amp;quot;] | None = None,\n    ) -&amp;gt; Float[Tensor, &amp;quot; ... queries d_v&amp;quot;]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Given key (K), query (Q), and value (V) tensors, return\n        the output of your scaled dot product attention implementation.\n    \n        Args:\n            Q (Float[Tensor, &amp;quot; ... queries d_k&amp;quot;]): Query tensor\n            K (Float[Tensor, &amp;quot; ... keys d_k&amp;quot;]): Key tensor\n            V (Float[Tensor, &amp;quot; ... values d_v&amp;quot;]): Values tensor\n            mask (Float[Tensor, &amp;quot; ... queries keys&amp;quot;] | None): Mask tensor\n        Returns:\n            Float[Tensor, &amp;quot; ... queries d_v&amp;quot;]: Output of SDPA\n        &amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       raise NotImplementedError\nE       NotImplementedError\n\ntests\\adapters.py:109: NotImplementedError\n&#34;}], &#34;tests/test_model.py::test_4d_scaled_dot_product_attention&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_model.py::test_4d_scaled_dot_product_attention&#34;, &#34;duration&#34;: &#34;00:00:01&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_model.py::test_4d_scaled_dot_product_attention&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:01&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;numpy_snapshot = &amp;lt;tests.conftest.NumpySnapshot object at 0x0000021B0B246A80&amp;gt;\nq = tensor([[[[-1.5256e+00, -7.5023e-01, -6.5398e-01,  ..., -8.6959e-01,\n           -3.3312e+00, -7.4787e-01],\n          [...825e-01],\n          [-1.8315e-01,  2.0009e+00,  1.4760e-01,  ...,  1.2993e+00,\n           -5.7777e-01,  1.4148e+00]]]])\nk = tensor([[[[-1.0408,  0.9166, -1.3042,  ...,  0.7787, -0.7749, -0.1398],\n          [ 1.1414, -0.6354, -1.4702,  ...,  1...,  0.2841,  ..., -0.6094,  0.1403,  1.3990],\n          [-0.1150,  0.0779,  1.3394,  ..., -0.3242, -0.8369,  0.8859]]]])\nv = tensor([[[[-0.0766,  0.3599, -0.7820,  ..., -0.5296,  1.3544,  1.3778],\n          [-0.0752, -0.4233,  0.4217,  ...,  2...,  1.1937,  ...,  0.4603, -0.9189,  0.2698],\n          [-0.7093, -1.5744,  1.5026,  ..., -1.7090,  0.2552,  0.8367]]]])\nmask = tensor([[[[ True,  True, False, False,  True, False, False, False, False, False,\n           False, False,  True, False... True, False, False, False,  True, False,  True, False,  True,\n           False, False, False, False, False, False]]]])\n\n    def test_4d_scaled_dot_product_attention(numpy_snapshot, q, k, v, mask):\n        # Shape: (batch_size, num_heads, seq_len, d_k)\n        q, k, v = (rearrange(x, &amp;quot;(batch head) seq d -&amp;gt; batch head seq d&amp;quot;, head=2) for x in (q, k, v))\n        mask = rearrange(mask, &amp;quot;(batch head) query key -&amp;gt; batch head query key&amp;quot;, head=2)\n    \n&amp;gt;       actual_output = run_scaled_dot_product_attention(Q=q, K=k, V=v, mask=mask)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_model.py:84: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nQ = tensor([[[[-1.5256e+00, -7.5023e-01, -6.5398e-01,  ..., -8.6959e-01,\n           -3.3312e+00, -7.4787e-01],\n          [...825e-01],\n          [-1.8315e-01,  2.0009e+00,  1.4760e-01,  ...,  1.2993e+00,\n           -5.7777e-01,  1.4148e+00]]]])\nK = tensor([[[[-1.0408,  0.9166, -1.3042,  ...,  0.7787, -0.7749, -0.1398],\n          [ 1.1414, -0.6354, -1.4702,  ...,  1...,  0.2841,  ..., -0.6094,  0.1403,  1.3990],\n          [-0.1150,  0.0779,  1.3394,  ..., -0.3242, -0.8369,  0.8859]]]])\nV = tensor([[[[-0.0766,  0.3599, -0.7820,  ..., -0.5296,  1.3544,  1.3778],\n          [-0.0752, -0.4233,  0.4217,  ...,  2...,  1.1937,  ...,  0.4603, -0.9189,  0.2698],\n          [-0.7093, -1.5744,  1.5026,  ..., -1.7090,  0.2552,  0.8367]]]])\nmask = tensor([[[[ True,  True, False, False,  True, False, False, False, False, False,\n           False, False,  True, False... True, False, False, False,  True, False,  True, False,  True,\n           False, False, False, False, False, False]]]])\n\n    def run_scaled_dot_product_attention(\n        Q: Float[Tensor, &amp;quot; ... queries d_k&amp;quot;],\n        K: Float[Tensor, &amp;quot; ... keys d_k&amp;quot;],\n        V: Float[Tensor, &amp;quot; ... values d_v&amp;quot;],\n        mask: Float[Tensor, &amp;quot; ... queries keys&amp;quot;] | None = None,\n    ) -&amp;gt; Float[Tensor, &amp;quot; ... queries d_v&amp;quot;]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Given key (K), query (Q), and value (V) tensors, return\n        the output of your scaled dot product attention implementation.\n    \n        Args:\n            Q (Float[Tensor, &amp;quot; ... queries d_k&amp;quot;]): Query tensor\n            K (Float[Tensor, &amp;quot; ... keys d_k&amp;quot;]): Key tensor\n            V (Float[Tensor, &amp;quot; ... values d_v&amp;quot;]): Values tensor\n            mask (Float[Tensor, &amp;quot; ... queries keys&amp;quot;] | None): Mask tensor\n        Returns:\n            Float[Tensor, &amp;quot; ... queries d_v&amp;quot;]: Output of SDPA\n        &amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       raise NotImplementedError\nE       NotImplementedError\n\ntests\\adapters.py:109: NotImplementedError\n&#34;}], &#34;tests/test_model.py::test_multihead_self_attention&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_model.py::test_multihead_self_attention&#34;, &#34;duration&#34;: &#34;7 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_model.py::test_multihead_self_attention&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;7 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;numpy_snapshot = &amp;lt;tests.conftest.NumpySnapshot object at 0x0000021B0B245CD0&amp;gt;\nin_embeddings = tensor([[[-0.9414,  1.2632, -0.1838,  ..., -0.1941,  0.0048, -1.3165],\n         [ 0.0204, -0.1652,  0.2109,  ...,  0.6...73, -0.5486,  ...,  1.5676, -1.1472, -1.5822],\n         [ 1.5936, -0.4113,  0.6037,  ..., -0.6494,  1.2858,  0.4321]]])\nd_model = 64, n_heads = 4\nts_state_dict = ({&amp;#x27;layers.0.attn.k_proj.weight&amp;#x27;: tensor([[ 0.0890,  0.1049,  0.0980,  ...,  0.1314,  0.0594, -0.0206],\n        [ 0.027...7,  ...,  0.0380, -0.0240,  0.1170]]), ...}, {&amp;#x27;context_length&amp;#x27;: 16, &amp;#x27;d_ff&amp;#x27;: 128, &amp;#x27;d_model&amp;#x27;: 64, &amp;#x27;ffn_type&amp;#x27;: None, ...})\n\n    def test_multihead_self_attention(numpy_snapshot, in_embeddings, d_model, n_heads, ts_state_dict):\n        d, _ = ts_state_dict\n        q_proj_weight, k_proj_weight, v_proj_weight, o_proj_weight = [\n            d[f&amp;quot;layers.0.attn.{k}_proj.weight&amp;quot;] for k in [&amp;quot;q&amp;quot;, &amp;quot;k&amp;quot;, &amp;quot;v&amp;quot;, &amp;quot;output&amp;quot;]\n        ]\n        # reference_weights = torch.load(FIXTURES_PATH / &amp;quot;unbatched_multihead_self_attention_weights.pt&amp;quot;)\n        # expected_output = torch.load(FIXTURES_PATH / &amp;quot;unbatched_multihead_self_attention_expected_output.pt&amp;quot;)\n&amp;gt;       actual_output = run_multihead_self_attention(\n            d_model=d_model,\n            num_heads=n_heads,\n            q_proj_weight=q_proj_weight,\n            k_proj_weight=k_proj_weight,\n            v_proj_weight=v_proj_weight,\n            o_proj_weight=o_proj_weight,\n            in_features=in_embeddings,\n        )\n\ntests\\test_model.py:98: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nd_model = 64, num_heads = 4\nq_proj_weight = tensor([[ 0.0372, -0.0888,  0.2558,  ...,  0.0049, -0.2693, -0.1108],\n        [ 0.0520,  0.0837, -0.0109,  ..., -0.006...1974, -0.1546,  ..., -0.1313,  0.0992, -0.1394],\n        [-0.0936,  0.2000, -0.0574,  ..., -0.0828,  0.1563,  0.1365]])\nk_proj_weight = tensor([[ 0.0890,  0.1049,  0.0980,  ...,  0.1314,  0.0594, -0.0206],\n        [ 0.0271,  0.0840, -0.2509,  ..., -0.325...1004,  0.1376,  ...,  0.0082, -0.0004,  0.1317],\n        [-0.2910, -0.0298, -0.1720,  ..., -0.1184,  0.0639, -0.0232]])\nv_proj_weight = tensor([[-0.0538, -0.0038, -0.0073,  ...,  0.1269,  0.0679,  0.0620],\n        [-0.0155, -0.0231,  0.0393,  ...,  0.100...1402,  0.0214,  ...,  0.0210,  0.0694, -0.0717],\n        [ 0.0523,  0.0327,  0.0257,  ...,  0.0380, -0.0240,  0.1170]])\no_proj_weight = tensor([[ 0.0676,  0.0116, -0.0078,  ..., -0.0227,  0.0357, -0.0327],\n        [ 0.0297,  0.1108, -0.1244,  ...,  0.081...0195,  0.0148,  ...,  0.0888,  0.0211,  0.0004],\n        [ 0.0432,  0.0543,  0.0269,  ..., -0.0546, -0.0417, -0.0533]])\nin_features = tensor([[[-0.9414,  1.2632, -0.1838,  ..., -0.1941,  0.0048, -1.3165],\n         [ 0.0204, -0.1652,  0.2109,  ...,  0.6...73, -0.5486,  ...,  1.5676, -1.1472, -1.5822],\n         [ 1.5936, -0.4113,  0.6037,  ..., -0.6494,  1.2858,  0.4321]]])\n\n    def run_multihead_self_attention(\n        d_model: int,\n        num_heads: int,\n        q_proj_weight: Float[Tensor, &amp;quot; d_k d_in&amp;quot;],\n        k_proj_weight: Float[Tensor, &amp;quot; d_k d_in&amp;quot;],\n        v_proj_weight: Float[Tensor, &amp;quot; d_v d_in&amp;quot;],\n        o_proj_weight: Float[Tensor, &amp;quot; d_model d_v&amp;quot;],\n        in_features: Float[Tensor, &amp;quot; ... sequence_length d_in&amp;quot;],\n    ) -&amp;gt; Float[Tensor, &amp;quot; ... sequence_length d_out&amp;quot;]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Given the key, query, and value projection weights of a naive unbatched\n        implementation of multi-head attention, return the output of an optimized batched\n        implementation. This implementation should handle the key, query, and value projections\n        for all heads in a single matrix multiply.\n        This function should not use RoPE.\n        See section 3.2.2 of Vaswani et al., 2017.\n    \n        Args:\n            d_model (int): Dimensionality of the feedforward input and output.\n            num_heads (int): Number of heads to use in multi-headed attention.\n            max_seq_len (int): Maximum sequence length to pre-cache if your implementation does that.\n            q_proj_weight (Float[Tensor, &amp;quot;d_k d_in&amp;quot;]): Weights for the Q projection\n            k_proj_weight (Float[Tensor, &amp;quot;d_k d_in&amp;quot;]): Weights for the K projection\n            v_proj_weight (Float[Tensor, &amp;quot;d_k d_in&amp;quot;]): Weights for the V projection\n            o_proj_weight (Float[Tensor, &amp;quot;d_model d_v&amp;quot;]): Weights for the output projection\n            in_features (Float[Tensor, &amp;quot;... sequence_length d_in&amp;quot;]): Tensor to run your implementation on.\n    \n        Returns:\n            Float[Tensor, &amp;quot; ... sequence_length d_out&amp;quot;]: Tensor with the output of running your optimized, batched multi-headed attention\n            implementation with the given QKV projection weights and input features.\n        &amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       raise NotImplementedError\nE       NotImplementedError\n\ntests\\adapters.py:143: NotImplementedError\n&#34;}], &#34;tests/test_model.py::test_multihead_self_attention_with_rope&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_model.py::test_multihead_self_attention_with_rope&#34;, &#34;duration&#34;: &#34;6 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_model.py::test_multihead_self_attention_with_rope&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;6 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;numpy_snapshot = &amp;lt;tests.conftest.NumpySnapshot object at 0x0000021B0EB22960&amp;gt;\nin_embeddings = tensor([[[-0.9414,  1.2632, -0.1838,  ..., -0.1941,  0.0048, -1.3165],\n         [ 0.0204, -0.1652,  0.2109,  ...,  0.6...73, -0.5486,  ...,  1.5676, -1.1472, -1.5822],\n         [ 1.5936, -0.4113,  0.6037,  ..., -0.6494,  1.2858,  0.4321]]])\nd_model = 64, n_heads = 4\nts_state_dict = ({&amp;#x27;layers.0.attn.k_proj.weight&amp;#x27;: tensor([[ 0.0890,  0.1049,  0.0980,  ...,  0.1314,  0.0594, -0.0206],\n        [ 0.027...7,  ...,  0.0380, -0.0240,  0.1170]]), ...}, {&amp;#x27;context_length&amp;#x27;: 16, &amp;#x27;d_ff&amp;#x27;: 128, &amp;#x27;d_model&amp;#x27;: 64, &amp;#x27;ffn_type&amp;#x27;: None, ...})\nn_keys = 16, theta = 10000.0, pos_ids = tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]])\n\n    def test_multihead_self_attention_with_rope(\n        numpy_snapshot, in_embeddings, d_model, n_heads, ts_state_dict, n_keys, theta, pos_ids\n    ):\n        d, _ = ts_state_dict\n        q_proj_weight, k_proj_weight, v_proj_weight, o_proj_weight = [\n            d[f&amp;quot;layers.0.attn.{k}_proj.weight&amp;quot;] for k in [&amp;quot;q&amp;quot;, &amp;quot;k&amp;quot;, &amp;quot;v&amp;quot;, &amp;quot;output&amp;quot;]\n        ]\n        # reference_weights = torch.load(FIXTURES_PATH / &amp;quot;unbatched_multihead_self_attention_weights.pt&amp;quot;)\n        # expected_output = torch.load(FIXTURES_PATH / &amp;quot;unbatched_multihead_self_attention_expected_output.pt&amp;quot;)\n        pos_ids = rearrange(pos_ids, &amp;quot;seq -&amp;gt; 1 seq&amp;quot;)\n&amp;gt;       actual_output = run_multihead_self_attention_with_rope(\n            d_model=d_model,\n            num_heads=n_heads,\n            max_seq_len=n_keys,\n            theta=theta,\n            q_proj_weight=q_proj_weight,\n            k_proj_weight=k_proj_weight,\n            v_proj_weight=v_proj_weight,\n            o_proj_weight=o_proj_weight,\n            in_features=in_embeddings,\n            token_positions=pos_ids,\n        )\n\ntests\\test_model.py:121: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nd_model = 64, num_heads = 4, max_seq_len = 16, theta = 10000.0\nq_proj_weight = tensor([[ 0.0372, -0.0888,  0.2558,  ...,  0.0049, -0.2693, -0.1108],\n        [ 0.0520,  0.0837, -0.0109,  ..., -0.006...1974, -0.1546,  ..., -0.1313,  0.0992, -0.1394],\n        [-0.0936,  0.2000, -0.0574,  ..., -0.0828,  0.1563,  0.1365]])\nk_proj_weight = tensor([[ 0.0890,  0.1049,  0.0980,  ...,  0.1314,  0.0594, -0.0206],\n        [ 0.0271,  0.0840, -0.2509,  ..., -0.325...1004,  0.1376,  ...,  0.0082, -0.0004,  0.1317],\n        [-0.2910, -0.0298, -0.1720,  ..., -0.1184,  0.0639, -0.0232]])\nv_proj_weight = tensor([[-0.0538, -0.0038, -0.0073,  ...,  0.1269,  0.0679,  0.0620],\n        [-0.0155, -0.0231,  0.0393,  ...,  0.100...1402,  0.0214,  ...,  0.0210,  0.0694, -0.0717],\n        [ 0.0523,  0.0327,  0.0257,  ...,  0.0380, -0.0240,  0.1170]])\no_proj_weight = tensor([[ 0.0676,  0.0116, -0.0078,  ..., -0.0227,  0.0357, -0.0327],\n        [ 0.0297,  0.1108, -0.1244,  ...,  0.081...0195,  0.0148,  ...,  0.0888,  0.0211,  0.0004],\n        [ 0.0432,  0.0543,  0.0269,  ..., -0.0546, -0.0417, -0.0533]])\nin_features = tensor([[[-0.9414,  1.2632, -0.1838,  ..., -0.1941,  0.0048, -1.3165],\n         [ 0.0204, -0.1652,  0.2109,  ...,  0.6...73, -0.5486,  ...,  1.5676, -1.1472, -1.5822],\n         [ 1.5936, -0.4113,  0.6037,  ..., -0.6494,  1.2858,  0.4321]]])\ntoken_positions = tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]])\n\n    def run_multihead_self_attention_with_rope(\n        d_model: int,\n        num_heads: int,\n        max_seq_len: int,\n        theta: float,\n        q_proj_weight: Float[Tensor, &amp;quot; d_k d_in&amp;quot;],\n        k_proj_weight: Float[Tensor, &amp;quot; d_k d_in&amp;quot;],\n        v_proj_weight: Float[Tensor, &amp;quot; d_v d_in&amp;quot;],\n        o_proj_weight: Float[Tensor, &amp;quot; d_model d_v&amp;quot;],\n        in_features: Float[Tensor, &amp;quot; ... sequence_length d_in&amp;quot;],\n        token_positions: Int[Tensor, &amp;quot; ... sequence_length&amp;quot;] | None = None,\n    ) -&amp;gt; Float[Tensor, &amp;quot; ... sequence_length d_out&amp;quot;]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Given the key, query, and value projection weights of a naive unbatched\n        implementation of multi-head attention, return the output of an optimized batched\n        implementation. This implementation should handle the key, query, and value projections\n        for all heads in a single matrix multiply.\n        This version of MHA should include RoPE.\n        In this case, the RoPE embedding dimension must be the head embedding dimension (d_model // num_heads).\n        See section 3.2.2 of Vaswani et al., 2017.\n    \n        Args:\n            d_model (int): Dimensionality of the feedforward input and output.\n            num_heads (int): Number of heads to use in multi-headed attention.\n            max_seq_len (int): Maximum sequence length to pre-cache if your implementation does that.\n            theta (float): RoPE parameter.\n            q_proj_weight (Float[Tensor, &amp;quot;d_k d_in&amp;quot;]): Weights for the Q projection\n            k_proj_weight (Float[Tensor, &amp;quot;d_k d_in&amp;quot;]): Weights for the K projection\n            v_proj_weight (Float[Tensor, &amp;quot;d_k d_in&amp;quot;]): Weights for the V projection\n            o_proj_weight (Float[Tensor, &amp;quot;d_model d_v&amp;quot;]): Weights for the output projection\n            in_features (Float[Tensor, &amp;quot;... sequence_length d_in&amp;quot;]): Tensor to run your implementation on.\n            token_positions (Int[Tensor, &amp;quot; ... sequence_length&amp;quot;] | None): Optional tensor with the positions of the tokens\n    \n        Returns:\n            Float[Tensor, &amp;quot; ... sequence_length d_out&amp;quot;]: Tensor with the output of running your optimized, batched multi-headed attention\n            implementation with the given QKV projection weights and input features.\n        &amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       raise NotImplementedError\nE       NotImplementedError\n\ntests\\adapters.py:183: NotImplementedError\n&#34;}], &#34;tests/test_model.py::test_transformer_lm&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_model.py::test_transformer_lm&#34;, &#34;duration&#34;: &#34;4 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_model.py::test_transformer_lm&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;4 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;numpy_snapshot = &amp;lt;tests.conftest.NumpySnapshot object at 0x0000021B0EB23290&amp;gt;, vocab_size = 10000, n_keys = 16\nd_model = 64, n_layers = 3, n_heads = 4, d_ff = 128, theta = 10000.0\nts_state_dict = ({&amp;#x27;layers.0.attn.k_proj.weight&amp;#x27;: tensor([[ 0.0890,  0.1049,  0.0980,  ...,  0.1314,  0.0594, -0.0206],\n        [ 0.027...7,  ...,  0.0380, -0.0240,  0.1170]]), ...}, {&amp;#x27;context_length&amp;#x27;: 16, &amp;#x27;d_ff&amp;#x27;: 128, &amp;#x27;d_model&amp;#x27;: 64, &amp;#x27;ffn_type&amp;#x27;: None, ...})\nin_indices = tensor([[5130, 8761, 2403, 3492, 2234, 1437, 5647, 7786, 1904, 6558, 9177, 7649],\n        [8235, 4285, 4217, 5482, 574... 861, 9322, 4143, 1833, 4502, 2050],\n        [ 907, 5061, 9813, 7793, 6212, 7893, 3454, 8112, 6322, 2511, 2434, 6879]])\n\n    def test_transformer_lm(\n        numpy_snapshot, vocab_size, n_keys, d_model, n_layers, n_heads, d_ff, theta, ts_state_dict, in_indices\n    ):\n        # reference_weights = torch.load(FIXTURES_PATH / &amp;quot;transformer_lm_weights.pt&amp;quot;)\n        # in_indices = torch.load(FIXTURES_PATH / &amp;quot;in_indices.pt&amp;quot;)\n        # expected_output = torch.load(FIXTURES_PATH / &amp;quot;transformer_lm_expected_output.pt&amp;quot;)\n        state_dict, _ = ts_state_dict\n    \n&amp;gt;       actual_output = run_transformer_lm(\n            vocab_size=vocab_size,\n            context_length=n_keys,\n            d_model=d_model,\n            num_layers=n_layers,\n            num_heads=n_heads,\n            d_ff=d_ff,\n            rope_theta=theta,\n            weights=state_dict,\n            in_indices=in_indices,\n        )\n\ntests\\test_model.py:145: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nvocab_size = 10000, context_length = 16, d_model = 64, num_layers = 3, num_heads = 4, d_ff = 128, rope_theta = 10000.0\nweights = {&amp;#x27;layers.0.attn.k_proj.weight&amp;#x27;: tensor([[ 0.0890,  0.1049,  0.0980,  ...,  0.1314,  0.0594, -0.0206],\n        [ 0.0271... 0.0214,  ...,  0.0210,  0.0694, -0.0717],\n        [ 0.0523,  0.0327,  0.0257,  ...,  0.0380, -0.0240,  0.1170]]), ...}\nin_indices = tensor([[5130, 8761, 2403, 3492, 2234, 1437, 5647, 7786, 1904, 6558, 9177, 7649],\n        [8235, 4285, 4217, 5482, 574... 861, 9322, 4143, 1833, 4502, 2050],\n        [ 907, 5061, 9813, 7793, 6212, 7893, 3454, 8112, 6322, 2511, 2434, 6879]])\n\n    def run_transformer_lm(\n        vocab_size: int,\n        context_length: int,\n        d_model: int,\n        num_layers: int,\n        num_heads: int,\n        d_ff: int,\n        rope_theta: float,\n        weights: dict[str, Tensor],\n        in_indices: Int[Tensor, &amp;quot; batch_size sequence_length&amp;quot;],\n    ) -&amp;gt; Float[Tensor, &amp;quot; batch_size sequence_length vocab_size&amp;quot;]:\n        &amp;quot;&amp;quot;&amp;quot;Given the weights of a Transformer language model and input indices,\n        return the output of running a forward pass on the input indices.\n    \n        This function should use RoPE.\n    \n        Args:\n            vocab_size (int): The number of unique items in the output vocabulary to be predicted.\n            context_length (int): The maximum number of tokens to process at once.\n            d_model (int): The dimensionality of the model embeddings and sublayer outputs.\n            num_layers (int): The number of Transformer layers to use.\n            num_heads (int): Number of heads to use in multi-headed attention. `d_model` must be\n                evenly divisible by `num_heads`.\n            d_ff (int): Dimensionality of the feed-forward inner layer (section 3.3).\n            rope_theta (float): The RoPE $\\Theta$ parameter.\n            weights (dict[str, Tensor]):\n                State dict of our reference implementation. {num_layers} refers to an\n                integer between `0` and `num_layers - 1` (the layer index).\n                The keys of this dictionary are:\n                - `token_embeddings.weight`\n                    Token embedding matrix. Shape is (vocab_size, d_model).\n                - `layers.{num_layers}.attn.q_proj.weight`\n                    The query projections for all `num_heads` attention heads.\n                    Shape is (num_heads * (d_model / num_heads), d_model).\n                    The rows are ordered by matrices of shape (num_heads, d_k),\n                    so `attn.q_proj.weight == torch.cat([q_heads.0.weight, ..., q_heads.N.weight], dim=0)`.\n                - `layers.{num_layers}.attn.k_proj.weight`\n                    The key projections for all `num_heads` attention heads.\n                    Shape is (num_heads * (d_model / num_heads), d_model).\n                    The rows are ordered by matrices of shape (num_heads, d_k),\n                    so `attn.k_proj.weight == torch.cat([k_heads.0.weight, ..., k_heads.N.weight], dim=0)`.\n                - `layers.{num_layers}.attn.v_proj.weight`\n                    The value projections for all `num_heads` attention heads.\n                    Shape is (num_heads * (d_model / num_heads), d_model).\n                    The rows are ordered by matrices of shape (num_heads, d_v),\n                    so `attn.v_proj.weight == torch.cat([v_heads.0.weight, ..., v_heads.N.weight], dim=0)`.\n                - `layers.{num_layers}.attn.output_proj.weight`\n                    Weight of the multi-head self-attention output projection\n                    Shape is ((d_model / num_heads) * num_heads, d_model).\n                - `layers.{num_layers}.ln1.weight`\n                    Weights of affine transform for the first RMSNorm\n                    applied in the transformer block.\n                    Shape is (d_model,).\n                - `layers.{num_layers}.ffn.w1.weight`\n                    Weight of the first linear transformation in the FFN.\n                    Shape is (d_model, d_ff).\n                - `layers.{num_layers}.ffn.w2.weight`\n                    Weight of the second linear transformation in the FFN.\n                    Shape is (d_ff, d_model).\n                - `layers.{num_layers}.ffn.w3.weight`\n                    Weight of the third linear transformation in the FFN.\n                    Shape is (d_model, d_ff).\n                - `layers.{num_layers}.ln2.weight`\n                    Weights of affine transform for the second RMSNorm\n                    applied in the transformer block.\n                    Shape is (d_model,).\n                - `ln_final.weight`\n                    Weights of affine transform for RMSNorm applied to the output of the final transformer block.\n                    Shape is (d_model, ).\n                - `lm_head.weight`\n                    Weights of the language model output embedding.\n                    Shape is (vocab_size, d_model).\n            in_indices (Int[Tensor, &amp;quot;batch_size sequence_length&amp;quot;]) Tensor with input indices to run the language model on. Shape is (batch_size, sequence_length), where\n                `sequence_length` is at most `context_length`.\n    \n        Returns:\n            Float[Tensor, &amp;quot;batch_size sequence_length vocab_size&amp;quot;]: Tensor with the predicted unnormalized\n            next-word distribution for each token.\n        &amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       raise NotImplementedError\nE       NotImplementedError\n\ntests\\adapters.py:360: NotImplementedError\n&#34;}], &#34;tests/test_model.py::test_transformer_lm_truncated_input&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_model.py::test_transformer_lm_truncated_input&#34;, &#34;duration&#34;: &#34;10 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_model.py::test_transformer_lm_truncated_input&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;10 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;numpy_snapshot = &amp;lt;tests.conftest.NumpySnapshot object at 0x0000021B0EB54320&amp;gt;, vocab_size = 10000, n_keys = 16\nd_model = 64, n_layers = 3, n_heads = 4, d_ff = 128, theta = 10000.0\nts_state_dict = ({&amp;#x27;layers.0.attn.k_proj.weight&amp;#x27;: tensor([[ 0.0890,  0.1049,  0.0980,  ...,  0.1314,  0.0594, -0.0206],\n        [ 0.027...7,  ...,  0.0380, -0.0240,  0.1170]]), ...}, {&amp;#x27;context_length&amp;#x27;: 16, &amp;#x27;d_ff&amp;#x27;: 128, &amp;#x27;d_model&amp;#x27;: 64, &amp;#x27;ffn_type&amp;#x27;: None, ...})\nin_indices = tensor([[5130, 8761, 2403, 3492, 2234, 1437, 5647, 7786, 1904, 6558, 9177, 7649],\n        [8235, 4285, 4217, 5482, 574... 861, 9322, 4143, 1833, 4502, 2050],\n        [ 907, 5061, 9813, 7793, 6212, 7893, 3454, 8112, 6322, 2511, 2434, 6879]])\n\n    def test_transformer_lm_truncated_input(\n        numpy_snapshot, vocab_size, n_keys, d_model, n_layers, n_heads, d_ff, theta, ts_state_dict, in_indices\n    ):\n        # reference_weights = torch.load(FIXTURES_PATH / &amp;quot;transformer_lm_weights.pt&amp;quot;)\n        # in_indices_truncated = torch.load(FIXTURES_PATH / &amp;quot;in_indices_truncated.pt&amp;quot;)\n        # truncated_expected_output = torch.load(FIXTURES_PATH / &amp;quot;transformer_lm_truncated_expected_output.pt&amp;quot;)\n&amp;gt;       truncated_actual_output = run_transformer_lm(\n            vocab_size=vocab_size,\n            context_length=n_keys,\n            d_model=d_model,\n            num_layers=n_layers,\n            num_heads=n_heads,\n            d_ff=d_ff,\n            rope_theta=theta,\n            weights=ts_state_dict[0],\n            in_indices=in_indices,\n        )\n\ntests\\test_model.py:170: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nvocab_size = 10000, context_length = 16, d_model = 64, num_layers = 3, num_heads = 4, d_ff = 128, rope_theta = 10000.0\nweights = {&amp;#x27;layers.0.attn.k_proj.weight&amp;#x27;: tensor([[ 0.0890,  0.1049,  0.0980,  ...,  0.1314,  0.0594, -0.0206],\n        [ 0.0271... 0.0214,  ...,  0.0210,  0.0694, -0.0717],\n        [ 0.0523,  0.0327,  0.0257,  ...,  0.0380, -0.0240,  0.1170]]), ...}\nin_indices = tensor([[5130, 8761, 2403, 3492, 2234, 1437, 5647, 7786, 1904, 6558, 9177, 7649],\n        [8235, 4285, 4217, 5482, 574... 861, 9322, 4143, 1833, 4502, 2050],\n        [ 907, 5061, 9813, 7793, 6212, 7893, 3454, 8112, 6322, 2511, 2434, 6879]])\n\n    def run_transformer_lm(\n        vocab_size: int,\n        context_length: int,\n        d_model: int,\n        num_layers: int,\n        num_heads: int,\n        d_ff: int,\n        rope_theta: float,\n        weights: dict[str, Tensor],\n        in_indices: Int[Tensor, &amp;quot; batch_size sequence_length&amp;quot;],\n    ) -&amp;gt; Float[Tensor, &amp;quot; batch_size sequence_length vocab_size&amp;quot;]:\n        &amp;quot;&amp;quot;&amp;quot;Given the weights of a Transformer language model and input indices,\n        return the output of running a forward pass on the input indices.\n    \n        This function should use RoPE.\n    \n        Args:\n            vocab_size (int): The number of unique items in the output vocabulary to be predicted.\n            context_length (int): The maximum number of tokens to process at once.\n            d_model (int): The dimensionality of the model embeddings and sublayer outputs.\n            num_layers (int): The number of Transformer layers to use.\n            num_heads (int): Number of heads to use in multi-headed attention. `d_model` must be\n                evenly divisible by `num_heads`.\n            d_ff (int): Dimensionality of the feed-forward inner layer (section 3.3).\n            rope_theta (float): The RoPE $\\Theta$ parameter.\n            weights (dict[str, Tensor]):\n                State dict of our reference implementation. {num_layers} refers to an\n                integer between `0` and `num_layers - 1` (the layer index).\n                The keys of this dictionary are:\n                - `token_embeddings.weight`\n                    Token embedding matrix. Shape is (vocab_size, d_model).\n                - `layers.{num_layers}.attn.q_proj.weight`\n                    The query projections for all `num_heads` attention heads.\n                    Shape is (num_heads * (d_model / num_heads), d_model).\n                    The rows are ordered by matrices of shape (num_heads, d_k),\n                    so `attn.q_proj.weight == torch.cat([q_heads.0.weight, ..., q_heads.N.weight], dim=0)`.\n                - `layers.{num_layers}.attn.k_proj.weight`\n                    The key projections for all `num_heads` attention heads.\n                    Shape is (num_heads * (d_model / num_heads), d_model).\n                    The rows are ordered by matrices of shape (num_heads, d_k),\n                    so `attn.k_proj.weight == torch.cat([k_heads.0.weight, ..., k_heads.N.weight], dim=0)`.\n                - `layers.{num_layers}.attn.v_proj.weight`\n                    The value projections for all `num_heads` attention heads.\n                    Shape is (num_heads * (d_model / num_heads), d_model).\n                    The rows are ordered by matrices of shape (num_heads, d_v),\n                    so `attn.v_proj.weight == torch.cat([v_heads.0.weight, ..., v_heads.N.weight], dim=0)`.\n                - `layers.{num_layers}.attn.output_proj.weight`\n                    Weight of the multi-head self-attention output projection\n                    Shape is ((d_model / num_heads) * num_heads, d_model).\n                - `layers.{num_layers}.ln1.weight`\n                    Weights of affine transform for the first RMSNorm\n                    applied in the transformer block.\n                    Shape is (d_model,).\n                - `layers.{num_layers}.ffn.w1.weight`\n                    Weight of the first linear transformation in the FFN.\n                    Shape is (d_model, d_ff).\n                - `layers.{num_layers}.ffn.w2.weight`\n                    Weight of the second linear transformation in the FFN.\n                    Shape is (d_ff, d_model).\n                - `layers.{num_layers}.ffn.w3.weight`\n                    Weight of the third linear transformation in the FFN.\n                    Shape is (d_model, d_ff).\n                - `layers.{num_layers}.ln2.weight`\n                    Weights of affine transform for the second RMSNorm\n                    applied in the transformer block.\n                    Shape is (d_model,).\n                - `ln_final.weight`\n                    Weights of affine transform for RMSNorm applied to the output of the final transformer block.\n                    Shape is (d_model, ).\n                - `lm_head.weight`\n                    Weights of the language model output embedding.\n                    Shape is (vocab_size, d_model).\n            in_indices (Int[Tensor, &amp;quot;batch_size sequence_length&amp;quot;]) Tensor with input indices to run the language model on. Shape is (batch_size, sequence_length), where\n                `sequence_length` is at most `context_length`.\n    \n        Returns:\n            Float[Tensor, &amp;quot;batch_size sequence_length vocab_size&amp;quot;]: Tensor with the predicted unnormalized\n            next-word distribution for each token.\n        &amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       raise NotImplementedError\nE       NotImplementedError\n\ntests\\adapters.py:360: NotImplementedError\n&#34;}], &#34;tests/test_model.py::test_transformer_block&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_model.py::test_transformer_block&#34;, &#34;duration&#34;: &#34;9 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_model.py::test_transformer_block&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;9 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;numpy_snapshot = &amp;lt;tests.conftest.NumpySnapshot object at 0x0000021B0EB55FD0&amp;gt;\nts_state_dict = ({&amp;#x27;layers.0.attn.k_proj.weight&amp;#x27;: tensor([[ 0.0890,  0.1049,  0.0980,  ...,  0.1314,  0.0594, -0.0206],\n        [ 0.027...7,  ...,  0.0380, -0.0240,  0.1170]]), ...}, {&amp;#x27;context_length&amp;#x27;: 16, &amp;#x27;d_ff&amp;#x27;: 128, &amp;#x27;d_model&amp;#x27;: 64, &amp;#x27;ffn_type&amp;#x27;: None, ...})\nin_embeddings = tensor([[[-0.9414,  1.2632, -0.1838,  ..., -0.1941,  0.0048, -1.3165],\n         [ 0.0204, -0.1652,  0.2109,  ...,  0.6...73, -0.5486,  ...,  1.5676, -1.1472, -1.5822],\n         [ 1.5936, -0.4113,  0.6037,  ..., -0.6494,  1.2858,  0.4321]]])\nd_model = 64, n_heads = 4, d_ff = 128, n_keys = 16, theta = 10000.0\n\n    def test_transformer_block(numpy_snapshot, ts_state_dict, in_embeddings, d_model, n_heads, d_ff, n_keys, theta):\n        # reference_weights = torch.load(FIXTURES_PATH / &amp;quot;transformer_block_weights.pt&amp;quot;)\n        # in_features = torch.load(FIXTURES_PATH / &amp;quot;in_features.pt&amp;quot;)\n    \n        block_weights = {k.replace(&amp;quot;layers.0.&amp;quot;, &amp;quot;&amp;quot;): v for k, v in ts_state_dict[0].items() if &amp;quot;layers.0.&amp;quot; in k}\n    \n&amp;gt;       actual_output = run_transformer_block(\n            d_model=d_model,\n            num_heads=n_heads,\n            d_ff=d_ff,\n            max_seq_len=n_keys,\n            theta=theta,\n            weights=block_weights,\n            in_features=in_embeddings,\n        )\n\ntests\\test_model.py:194: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nd_model = 64, num_heads = 4, d_ff = 128, max_seq_len = 16, theta = 10000.0\nweights = {&amp;#x27;attn.k_proj.weight&amp;#x27;: tensor([[ 0.0890,  0.1049,  0.0980,  ...,  0.1314,  0.0594, -0.0206],\n        [ 0.0271,  0.0840... 0.0214,  ...,  0.0210,  0.0694, -0.0717],\n        [ 0.0523,  0.0327,  0.0257,  ...,  0.0380, -0.0240,  0.1170]]), ...}\nin_features = tensor([[[-0.9414,  1.2632, -0.1838,  ..., -0.1941,  0.0048, -1.3165],\n         [ 0.0204, -0.1652,  0.2109,  ...,  0.6...73, -0.5486,  ...,  1.5676, -1.1472, -1.5822],\n         [ 1.5936, -0.4113,  0.6037,  ..., -0.6494,  1.2858,  0.4321]]])\n\n    def run_transformer_block(\n        d_model: int,\n        num_heads: int,\n        d_ff: int,\n        max_seq_len: int,\n        theta: float,\n        weights: dict[str, Tensor],\n        in_features: Float[Tensor, &amp;quot; batch sequence_length d_model&amp;quot;],\n    ) -&amp;gt; Float[Tensor, &amp;quot; batch sequence_length d_model&amp;quot;]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Given the weights of a pre-norm Transformer block and input features,\n        return the output of running the Transformer block on the input features.\n    \n        This function should use RoPE.\n        Depending on your implementation, you may simply need to pass the relevant args\n        to your TransformerBlock constructor, or you may need to initialize your own RoPE\n        class and pass that instead.\n    \n        Args:\n            d_model (int): The dimensionality of the Transformer block input.\n            num_heads (int): Number of heads to use in multi-headed attention. `d_model` must be\n                evenly divisible by `num_heads`.\n            d_ff (int): Dimensionality of the feed-forward inner layer.\n            max_seq_len (int): Maximum sequence length to pre-cache if your implementation does that.\n            theta (float): RoPE parameter.\n            weights (dict[str, Tensor]):\n                State dict of our reference implementation.\n                The keys of this dictionary are:\n                - `attn.q_proj.weight`\n                    The query projections for all `num_heads` attention heads.\n                    Shape is (d_model, d_model).\n                    The rows are ordered by matrices of shape (num_heads, d_k),\n                    so `attn.q_proj.weight == torch.cat([q_heads.0.weight, ..., q_heads.N.weight], dim=0)`.\n                - `attn.k_proj.weight`\n                    The key projections for all `num_heads` attention heads.\n                    Shape is (d_model, d_model).\n                    The rows are ordered by matrices of shape (num_heads, d_k),\n                    so `attn.k_proj.weight == torch.cat([k_heads.0.weight, ..., k_heads.N.weight], dim=0)`.\n                - `attn.v_proj.weight`\n                    The value projections for all `num_heads` attention heads.\n                    Shape is (d_model, d_model).\n                    The rows are ordered by matrices of shape (num_heads, d_v),\n                    so `attn.v_proj.weight == torch.cat([v_heads.0.weight, ..., v_heads.N.weight], dim=0)`.\n                - `attn.output_proj.weight`\n                    Weight of the multi-head self-attention output projection\n                    Shape is (d_model, d_model).\n                - `ln1.weight`\n                    Weights of affine transform for the first RMSNorm\n                    applied in the transformer block.\n                    Shape is (d_model,).\n                - `ffn.w1.weight`\n                    Weight of the first linear transformation in the FFN.\n                    Shape is (d_model, d_ff).\n                - `ffn.w2.weight`\n                    Weight of the second linear transformation in the FFN.\n                    Shape is (d_ff, d_model).\n                - `ffn.w3.weight`\n                    Weight of the third linear transformation in the FFN.\n                    Shape is (d_model, d_ff).\n                - `ln2.weight`\n                    Weights of affine transform for the second RMSNorm\n                    applied in the transformer block.\n                    Shape is (d_model,).\n            in_features (Float[Tensor, &amp;quot;batch sequence_length d_model&amp;quot;]):\n                Tensor to run your implementation on.\n    \n        Returns:\n            Float[Tensor, &amp;quot;batch sequence_length d_model&amp;quot;] Tensor with the output of\n            running the Transformer block on the input features while using RoPE.\n        &amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       raise NotImplementedError\nE       NotImplementedError\n\ntests\\adapters.py:278: NotImplementedError\n&#34;}], &#34;tests/test_model.py::test_rmsnorm&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_model.py::test_rmsnorm&#34;, &#34;duration&#34;: &#34;9 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_model.py::test_rmsnorm&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;9 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;numpy_snapshot = &amp;lt;tests.conftest.NumpySnapshot object at 0x0000021B0B246810&amp;gt;\nts_state_dict = ({&amp;#x27;layers.0.attn.k_proj.weight&amp;#x27;: tensor([[ 0.0890,  0.1049,  0.0980,  ...,  0.1314,  0.0594, -0.0206],\n        [ 0.027...7,  ...,  0.0380, -0.0240,  0.1170]]), ...}, {&amp;#x27;context_length&amp;#x27;: 16, &amp;#x27;d_ff&amp;#x27;: 128, &amp;#x27;d_model&amp;#x27;: 64, &amp;#x27;ffn_type&amp;#x27;: None, ...})\nin_embeddings = tensor([[[-0.9414,  1.2632, -0.1838,  ..., -0.1941,  0.0048, -1.3165],\n         [ 0.0204, -0.1652,  0.2109,  ...,  0.6...73, -0.5486,  ...,  1.5676, -1.1472, -1.5822],\n         [ 1.5936, -0.4113,  0.6037,  ..., -0.6494,  1.2858,  0.4321]]])\n\n    def test_rmsnorm(numpy_snapshot, ts_state_dict, in_embeddings):\n        state_dict, _ = ts_state_dict\n        reference_weights = state_dict[&amp;quot;layers.1.ln1.weight&amp;quot;]\n        d_model = reference_weights.shape[0]\n        # reference_weights = torch.load(FIXTURES_PATH / &amp;quot;rmsnorm_weights.pt&amp;quot;)\n        # in_features = torch.load(FIXTURES_PATH / &amp;quot;in_features.pt&amp;quot;)\n        # expected_output = torch.load(FIXTURES_PATH / &amp;quot;rmsnorm_expected_output.pt&amp;quot;)\n        # actual_output = run_rmsnorm(d_model=d_model, eps=1e-5, weights=reference_weights, in_features=in_features)\n    \n        # in_features = torch.randn()\n    \n&amp;gt;       actual_output = run_rmsnorm(d_model=d_model, eps=1e-5, weights=reference_weights, in_features=in_embeddings)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_model.py:220: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nd_model = 64, eps = 1e-05\nweights = tensor([0.9050, 0.8366, 0.7248, 0.6513, 0.8690, 0.7719, 0.8492, 0.8615, 0.6426,\n        0.9254, 0.7387, 0.7531, 0.9166....8457, 0.8530, 0.8500,\n        0.7135, 0.6711, 0.7081, 0.8161, 0.9134, 0.7955, 0.7472, 0.6888, 0.7708,\n        0.6498])\nin_features = tensor([[[-0.9414,  1.2632, -0.1838,  ..., -0.1941,  0.0048, -1.3165],\n         [ 0.0204, -0.1652,  0.2109,  ...,  0.6...73, -0.5486,  ...,  1.5676, -1.1472, -1.5822],\n         [ 1.5936, -0.4113,  0.6037,  ..., -0.6494,  1.2858,  0.4321]]])\n\n    def run_rmsnorm(\n        d_model: int,\n        eps: float,\n        weights: Float[Tensor, &amp;quot; d_model&amp;quot;],\n        in_features: Float[Tensor, &amp;quot; ... d_model&amp;quot;],\n    ) -&amp;gt; Float[Tensor, &amp;quot; ... d_model&amp;quot;]:\n        &amp;quot;&amp;quot;&amp;quot;Given the weights of a RMSNorm affine transform,\n        return the output of running RMSNorm on the input features.\n    \n        Args:\n            d_model (int): The dimensionality of the RMSNorm input.\n            eps: (float): A value added to the denominator for numerical stability.\n            weights (Float[Tensor, &amp;quot;d_model&amp;quot;]): RMSNorm weights.\n            in_features (Float[Tensor, &amp;quot;... d_model&amp;quot;]): Input features to run RMSNorm on. Can have arbitrary leading\n                dimensions.\n    \n        Returns:\n            Float[Tensor,&amp;quot;... d_model&amp;quot;]: Tensor of with the same shape as `in_features` with the output of running\n            RMSNorm of the `in_features`.\n        &amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       raise NotImplementedError\nE       NotImplementedError\n\ntests\\adapters.py:383: NotImplementedError\n&#34;}], &#34;tests/test_model.py::test_rope&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_model.py::test_rope&#34;, &#34;duration&#34;: &#34;2 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_model.py::test_rope&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;2 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;numpy_snapshot = &amp;lt;tests.conftest.NumpySnapshot object at 0x0000021B0EB57920&amp;gt;\nin_embeddings = tensor([[[-0.9414,  1.2632, -0.1838,  ..., -0.1941,  0.0048, -1.3165],\n         [ 0.0204, -0.1652,  0.2109,  ...,  0.6...73, -0.5486,  ...,  1.5676, -1.1472, -1.5822],\n         [ 1.5936, -0.4113,  0.6037,  ..., -0.6494,  1.2858,  0.4321]]])\nd_model = 64, theta = 10000.0, n_queries = 12, pos_ids = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n\n    def test_rope(numpy_snapshot, in_embeddings, d_model, theta, n_queries, pos_ids):\n&amp;gt;       output = run_rope(\n            d_model, theta=theta, max_seq_len=n_queries, in_query_or_key=in_embeddings, token_positions=pos_ids\n        )\n\ntests\\test_model.py:226: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nd_k = 64, theta = 10000.0, max_seq_len = 12\nin_query_or_key = tensor([[[-0.9414,  1.2632, -0.1838,  ..., -0.1941,  0.0048, -1.3165],\n         [ 0.0204, -0.1652,  0.2109,  ...,  0.6...73, -0.5486,  ...,  1.5676, -1.1472, -1.5822],\n         [ 1.5936, -0.4113,  0.6037,  ..., -0.6494,  1.2858,  0.4321]]])\ntoken_positions = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n\n    def run_rope(\n        d_k: int,\n        theta: float,\n        max_seq_len: int,\n        in_query_or_key: Float[Tensor, &amp;quot; ... sequence_length d_k&amp;quot;],\n        token_positions: Int[Tensor, &amp;quot; ... sequence_length&amp;quot;],\n    ) -&amp;gt; Float[Tensor, &amp;quot; ... sequence_length d_k&amp;quot;]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Run RoPE for a given input tensor.\n    \n        Args:\n            d_k (int): Embedding dimension size for the query or key tensor.\n            theta (float): RoPE parameter.\n            max_seq_len (int): Maximum sequence length to pre-cache if your implementation does that.\n            in_query_or_key (Float[Tensor, &amp;quot;... sequence_length d_k&amp;quot;]): Input tensor to run RoPE on.\n            token_positions (Int[Tensor, &amp;quot;... sequence_length&amp;quot;]): Tensor of shape (batch_size, sequence_length) with the token positions\n        Returns:\n            Float[Tensor, &amp;quot; ... sequence_length d_k&amp;quot;]: Tensor with RoPEd input.\n        &amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       raise NotImplementedError\nE       NotImplementedError\n\ntests\\adapters.py:205: NotImplementedError\n&#34;}], &#34;tests/test_model.py::test_silu_matches_pytorch&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_model.py::test_silu_matches_pytorch&#34;, &#34;duration&#34;: &#34;4 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_model.py::test_silu_matches_pytorch&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;4 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_silu_matches_pytorch():\n        x = torch.tensor(\n            [\n                [0.2352, 0.9259, 0.5189, 0.4725, 0.9730],\n                [0.7581, 0.9692, 0.2129, 0.9345, 0.0149],\n            ]\n        )\n        expected_output = F.silu(x)\n&amp;gt;       actual_output = run_silu(x)\n                        ^^^^^^^^^^^\n\ntests\\test_model.py:240: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nin_features = tensor([[0.2352, 0.9259, 0.5189, 0.4725, 0.9730],\n        [0.7581, 0.9692, 0.2129, 0.9345, 0.0149]])\n\n    def run_silu(in_features: Float[Tensor, &amp;quot; ...&amp;quot;]) -&amp;gt; Float[Tensor, &amp;quot; ...&amp;quot;]:\n        &amp;quot;&amp;quot;&amp;quot;Given a tensor of inputs, return the output of applying SiLU\n        to each element.\n    \n        Args:\n            in_features(Float[Tensor, &amp;quot;...&amp;quot;]): Input features to run SiLU on. Shape is arbitrary.\n    \n        Returns:\n            Float[Tensor,&amp;quot;...&amp;quot;]: of with the same shape as `in_features` with the output of applying\n            SiLU to each element.\n        &amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       raise NotImplementedError\nE       NotImplementedError\n\ntests\\adapters.py:397: NotImplementedError\n&#34;}], &#34;tests/test_nn_utils.py::test_softmax_matches_pytorch&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_nn_utils.py::test_softmax_matches_pytorch&#34;, &#34;duration&#34;: &#34;11 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_nn_utils.py::test_softmax_matches_pytorch&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;11 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_softmax_matches_pytorch():\n        x = torch.tensor(\n            [\n                [0.4655, 0.8303, 0.9608, 0.9656, 0.6840],\n                [0.2583, 0.2198, 0.9334, 0.2995, 0.1722],\n                [0.1573, 0.6860, 0.1327, 0.7284, 0.6811],\n            ]\n        )\n        expected = F.softmax(x, dim=-1)\n&amp;gt;       numpy.testing.assert_allclose(run_softmax(x, dim=-1).detach().numpy(), expected.detach().numpy(), atol=1e-6)\n                                      ^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_nn_utils.py:17: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nin_features = tensor([[0.4655, 0.8303, 0.9608, 0.9656, 0.6840],\n        [0.2583, 0.2198, 0.9334, 0.2995, 0.1722],\n        [0.1573, 0.6860, 0.1327, 0.7284, 0.6811]])\ndim = -1\n\n    def run_softmax(in_features: Float[Tensor, &amp;quot; ...&amp;quot;], dim: int) -&amp;gt; Float[Tensor, &amp;quot; ...&amp;quot;]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Given a tensor of inputs, return the output of softmaxing the given `dim`\n        of the input.\n    \n        Args:\n            in_features (Float[Tensor, &amp;quot;...&amp;quot;]): Input features to softmax. Shape is arbitrary.\n            dim (int): Dimension of the `in_features` to apply softmax to.\n    \n        Returns:\n            Float[Tensor, &amp;quot;...&amp;quot;]: Tensor of with the same shape as `in_features` with the output of\n            softmax normalizing the specified `dim`.\n        &amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       raise NotImplementedError\nE       NotImplementedError\n\ntests\\adapters.py:436: NotImplementedError\n&#34;}], &#34;tests/test_nn_utils.py::test_cross_entropy&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_nn_utils.py::test_cross_entropy&#34;, &#34;duration&#34;: &#34;4 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_nn_utils.py::test_cross_entropy&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;4 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_cross_entropy():\n        inputs = torch.tensor(\n            [\n                [\n                    [0.1088, 0.1060, 0.6683, 0.5131, 0.0645],\n                    [0.4538, 0.6852, 0.2520, 0.3792, 0.2675],\n                    [0.4578, 0.3357, 0.6384, 0.0481, 0.5612],\n                    [0.9639, 0.8864, 0.1585, 0.3038, 0.0350],\n                ],\n                [\n                    [0.3356, 0.9013, 0.7052, 0.8294, 0.8334],\n                    [0.6333, 0.4434, 0.1428, 0.5739, 0.3810],\n                    [0.9476, 0.5917, 0.7037, 0.2987, 0.6208],\n                    [0.8541, 0.1803, 0.2054, 0.4775, 0.8199],\n                ],\n            ]\n        )\n        targets = torch.tensor([[1, 0, 2, 2], [4, 1, 4, 0]])\n        expected = F.cross_entropy(inputs.view(-1, inputs.size(-1)), targets.view(-1))\n        numpy.testing.assert_allclose(\n&amp;gt;           run_cross_entropy(inputs.view(-1, inputs.size(-1)), targets.view(-1)).detach().numpy(),\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n            expected.detach().numpy(),\n            atol=1e-4,\n        )\n\ntests\\test_nn_utils.py:46: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ninputs = tensor([[0.1088, 0.1060, 0.6683, 0.5131, 0.0645],\n        [0.4538, 0.6852, 0.2520, 0.3792, 0.2675],\n        [0.4578, 0..., 0.5739, 0.3810],\n        [0.9476, 0.5917, 0.7037, 0.2987, 0.6208],\n        [0.8541, 0.1803, 0.2054, 0.4775, 0.8199]])\ntargets = tensor([1, 0, 2, 2, 4, 1, 4, 0])\n\n    def run_cross_entropy(inputs: Float[Tensor, &amp;quot; batch_size vocab_size&amp;quot;], targets: Int[Tensor, &amp;quot; batch_size&amp;quot;]) -&amp;gt; Float[Tensor, &amp;quot;&amp;quot;]:\n        &amp;quot;&amp;quot;&amp;quot;Given a tensor of inputs and targets, compute the average cross-entropy\n        loss across examples.\n    \n        Args:\n            inputs (Float[Tensor, &amp;quot;batch_size vocab_size&amp;quot;]): inputs[i][j] is the\n                unnormalized logit of jth class for the ith example.\n            targets (Int[Tensor, &amp;quot;batch_size&amp;quot;]): Tensor of shape (batch_size,) with the index of the correct class.\n                Each value must be between 0 and `num_classes - 1`.\n    \n        Returns:\n            Float[Tensor, &amp;quot;&amp;quot;]: The average cross-entropy loss across examples.\n        &amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       raise NotImplementedError\nE       NotImplementedError\n\ntests\\adapters.py:452: NotImplementedError\n&#34;}], &#34;tests/test_nn_utils.py::test_gradient_clipping&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_nn_utils.py::test_gradient_clipping&#34;, &#34;duration&#34;: &#34;38 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_nn_utils.py::test_gradient_clipping&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;38 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_gradient_clipping():\n        tensors = [torch.randn((5, 5)) for _ in range(6)]\n        max_norm = 1e-2\n    \n        t1 = tuple(torch.nn.Parameter(torch.clone(t)) for t in tensors)\n        # Test freezing one parameter.\n        t1[-1].requires_grad_(False)\n    \n        loss = torch.cat(t1).sum()\n        loss.backward()\n        torch.nn.utils.clip_grad.clip_grad_norm_(t1, max_norm)\n        t1_grads = [torch.clone(t.grad) for t in t1 if t.grad is not None]\n    \n        t1_c = tuple(torch.nn.Parameter(torch.clone(t)) for t in tensors)\n        t1_c[-1].requires_grad_(False)\n        loss_c = torch.cat(t1_c).sum()\n        loss_c.backward()\n&amp;gt;       run_gradient_clipping(t1_c, max_norm)\n\ntests\\test_nn_utils.py:78: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nparameters = (Parameter containing:\ntensor([[-1.1690, -1.0330,  0.1898, -0.9892,  0.2509],\n        [-1.6767,  1.4562,  0.9681, -0.2....9786],\n        [-0.3197,  1.8108, -1.2725, -1.1709,  0.8299],\n        [ 2.0029,  1.7995, -0.2501,  0.1244, -0.4229]]))\nmax_l2_norm = 0.01\n\n    def run_gradient_clipping(parameters: Iterable[torch.nn.Parameter], max_l2_norm: float) -&amp;gt; None:\n        &amp;quot;&amp;quot;&amp;quot;Given a set of parameters, clip their combined gradients to have l2 norm at most max_l2_norm.\n    \n        Args:\n            parameters (Iterable[torch.nn.Parameter]): collection of trainable parameters.\n            max_l2_norm (float): a positive value containing the maximum l2-norm.\n    \n        The gradients of the parameters (parameter.grad) should be modified in-place.\n        &amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       raise NotImplementedError\nE       NotImplementedError\n\ntests\\adapters.py:464: NotImplementedError\n&#34;}], &#34;tests/test_optimizer.py::test_adamw&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_optimizer.py::test_adamw&#34;, &#34;duration&#34;: &#34;444 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_optimizer.py::test_adamw&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;444 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;numpy_snapshot = &amp;lt;tests.conftest.NumpySnapshot object at 0x0000021B0EBCB170&amp;gt;\n\n    def test_adamw(numpy_snapshot):\n        &amp;quot;&amp;quot;&amp;quot;\n        Our reference implementation yields slightly different results than the\n        PyTorch AdamW, since there are a couple different ways that you can apply\n        weight decay that are equivalent in principle, but differ in practice due to\n        floating point behavior. So, we test that the provided implementation matches\n        _either_ our reference implementation&amp;#x27;s expected results or those from the PyTorch AdamW.\n        &amp;quot;&amp;quot;&amp;quot;\n        # expected_weights = torch.load(FIXTURES_PATH / &amp;quot;adamw_expected_params.pt&amp;quot;)\n        pytorch_weights = _optimize(torch.optim.AdamW)\n&amp;gt;       actual_weights = _optimize(get_adamw_cls())\n                                   ^^^^^^^^^^^^^^^\n\ntests\\test_optimizer.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    def get_adamw_cls() -&amp;gt; type[torch.optim.Optimizer]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Returns a torch.optim.Optimizer that implements AdamW.\n        &amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       raise NotImplementedError\nE       NotImplementedError\n\ntests\\adapters.py:471: NotImplementedError\n&#34;}], &#34;tests/test_optimizer.py::test_get_lr_cosine_schedule&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_optimizer.py::test_get_lr_cosine_schedule&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_optimizer.py::test_get_lr_cosine_schedule&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_get_lr_cosine_schedule():\n        max_learning_rate = 1\n        min_learning_rate = 1 * 0.1\n        warmup_iters = 7\n        cosine_cycle_iters = 21\n    \n        expected_lrs = [\n            0,\n            0.14285714285714285,\n            0.2857142857142857,\n            0.42857142857142855,\n            0.5714285714285714,\n            0.7142857142857143,\n            0.8571428571428571,\n            1.0,\n            0.9887175604818206,\n            0.9554359905560885,\n            0.9018241671106134,\n            0.8305704108364301,\n            0.7452476826029011,\n            0.6501344202803414,\n            0.55,\n            0.44986557971965857,\n            0.3547523173970989,\n            0.26942958916356996,\n            0.19817583288938662,\n            0.14456400944391146,\n            0.11128243951817937,\n            0.1,\n            0.1,\n            0.1,\n            0.1,\n        ]\n        actual_lrs = [\n&amp;gt;           run_get_lr_cosine_schedule(\n                it=it,\n                max_learning_rate=max_learning_rate,\n                min_learning_rate=min_learning_rate,\n                warmup_iters=warmup_iters,\n                cosine_cycle_iters=cosine_cycle_iters,\n            )\n            for it in range(25)\n        ]\n\ntests\\test_optimizer.py:87: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nit = 0, max_learning_rate = 1, min_learning_rate = 0.1, warmup_iters = 7, cosine_cycle_iters = 21\n\n    def run_get_lr_cosine_schedule(\n        it: int,\n        max_learning_rate: float,\n        min_learning_rate: float,\n        warmup_iters: int,\n        cosine_cycle_iters: int,\n    ):\n        &amp;quot;&amp;quot;&amp;quot;\n        Given the parameters of a cosine learning rate decay schedule (with linear\n        warmup) and an iteration number, return the learning rate at the given\n        iteration under the specified schedule.\n    \n        Args:\n            it (int): Iteration number to get learning rate for.\n            max_learning_rate (float): alpha_max, the maximum learning rate for\n                cosine learning rate schedule (with warmup).\n            min_learning_rate (float): alpha_min, the minimum / final learning rate for\n                the cosine learning rate schedule (with warmup).\n            warmup_iters (int): T_w, the number of iterations to linearly warm-up\n                the learning rate.\n            cosine_cycle_iters (int): T_c, the number of cosine annealing iterations.\n    \n        Returns:\n            Learning rate at the given iteration under the specified schedule.\n        &amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       raise NotImplementedError\nE       NotImplementedError\n\ntests\\adapters.py:499: NotImplementedError\n&#34;}], &#34;tests/test_serialization.py::test_checkpointing&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_serialization.py::test_checkpointing&#34;, &#34;duration&#34;: &#34;15 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_serialization.py::test_checkpointing&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;15 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;tmp_path = WindowsPath(&amp;#x27;C:/Users/jrsta/AppData/Local/Temp/pytest-of-jrsta/pytest-1/test_checkpointing0&amp;#x27;)\n\n    def test_checkpointing(tmp_path):\n        torch.manual_seed(42)\n        d_input = 100\n        d_output = 10\n        num_iters = 10\n    \n        model = _TestNet(d_input=d_input, d_output=d_output)\n&amp;gt;       optimizer = get_adamw_cls()(\n                    ^^^^^^^^^^^^^^^\n            model.parameters(),\n            lr=1e-3,\n            weight_decay=0.01,\n            betas=(0.9, 0.999),\n            eps=1e-8,\n        )\n\ntests\\test_serialization.py:64: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n    def get_adamw_cls() -&amp;gt; type[torch.optim.Optimizer]:\n        &amp;quot;&amp;quot;&amp;quot;\n        Returns a torch.optim.Optimizer that implements AdamW.\n        &amp;quot;&amp;quot;&amp;quot;\n&amp;gt;       raise NotImplementedError\nE       NotImplementedError\n\ntests\\adapters.py:471: NotImplementedError\n&#34;}], &#34;tests/test_tokenizer.py::test_roundtrip_empty&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_roundtrip_empty&#34;, &#34;duration&#34;: &#34;51 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_roundtrip_empty&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;51 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_roundtrip_empty():\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH,\n            merges_path=MERGES_PATH,\n        )\n\ntests\\test_tokenizer.py:78: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B16992E70&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_empty_matches_tiktoken&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_empty_matches_tiktoken&#34;, &#34;duration&#34;: &#34;212 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_empty_matches_tiktoken&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;212 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_empty_matches_tiktoken():\n        reference_tokenizer = tiktoken.get_encoding(&amp;quot;gpt2&amp;quot;)\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH,\n            merges_path=MERGES_PATH,\n        )\n\ntests\\test_tokenizer.py:90: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B167C9520&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_roundtrip_single_character&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_roundtrip_single_character&#34;, &#34;duration&#34;: &#34;16 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_roundtrip_single_character&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;16 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_roundtrip_single_character():\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH,\n            merges_path=MERGES_PATH,\n        )\n\ntests\\test_tokenizer.py:108: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B186F99A0&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_single_character_matches_tiktoken&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_single_character_matches_tiktoken&#34;, &#34;duration&#34;: &#34;24 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_single_character_matches_tiktoken&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;24 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_single_character_matches_tiktoken():\n        reference_tokenizer = tiktoken.get_encoding(&amp;quot;gpt2&amp;quot;)\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH,\n            merges_path=MERGES_PATH,\n        )\n\ntests\\test_tokenizer.py:120: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B18809D90&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_roundtrip_single_unicode_character&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_roundtrip_single_unicode_character&#34;, &#34;duration&#34;: &#34;26 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_roundtrip_single_unicode_character&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;26 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_roundtrip_single_unicode_character():\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH,\n            merges_path=MERGES_PATH,\n        )\n\ntests\\test_tokenizer.py:138: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B17536180&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_single_unicode_character_matches_tiktoken&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_single_unicode_character_matches_tiktoken&#34;, &#34;duration&#34;: &#34;23 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_single_unicode_character_matches_tiktoken&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;23 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_single_unicode_character_matches_tiktoken():\n        reference_tokenizer = tiktoken.get_encoding(&amp;quot;gpt2&amp;quot;)\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH,\n            merges_path=MERGES_PATH,\n        )\n\ntests\\test_tokenizer.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B1694E600&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_roundtrip_ascii_string&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_roundtrip_ascii_string&#34;, &#34;duration&#34;: &#34;23 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_roundtrip_ascii_string&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;23 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_roundtrip_ascii_string():\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH,\n            merges_path=MERGES_PATH,\n        )\n\ntests\\test_tokenizer.py:165: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B186555E0&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_ascii_string_matches_tiktoken&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_ascii_string_matches_tiktoken&#34;, &#34;duration&#34;: &#34;27 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_ascii_string_matches_tiktoken&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;27 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_ascii_string_matches_tiktoken():\n        reference_tokenizer = tiktoken.get_encoding(&amp;quot;gpt2&amp;quot;)\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH, merges_path=MERGES_PATH, special_tokens=[&amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;]\n        )\n\ntests\\test_tokenizer.py:177: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B19BEEF00&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_roundtrip_unicode_string&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_roundtrip_unicode_string&#34;, &#34;duration&#34;: &#34;24 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_roundtrip_unicode_string&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;24 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_roundtrip_unicode_string():\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH,\n            merges_path=MERGES_PATH,\n        )\n\ntests\\test_tokenizer.py:194: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B187B3320&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_unicode_string_matches_tiktoken&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_unicode_string_matches_tiktoken&#34;, &#34;duration&#34;: &#34;25 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_unicode_string_matches_tiktoken&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;25 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_unicode_string_matches_tiktoken():\n        reference_tokenizer = tiktoken.get_encoding(&amp;quot;gpt2&amp;quot;)\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH, merges_path=MERGES_PATH, special_tokens=[&amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;]\n        )\n\ntests\\test_tokenizer.py:206: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B1688F710&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_roundtrip_unicode_string_with_special_tokens&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_roundtrip_unicode_string_with_special_tokens&#34;, &#34;duration&#34;: &#34;24 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_roundtrip_unicode_string_with_special_tokens&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;24 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_roundtrip_unicode_string_with_special_tokens():\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH, merges_path=MERGES_PATH, special_tokens=[&amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;]\n        )\n\ntests\\test_tokenizer.py:220: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B18867B60&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_unicode_string_with_special_tokens_matches_tiktoken&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_unicode_string_with_special_tokens_matches_tiktoken&#34;, &#34;duration&#34;: &#34;22 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_unicode_string_with_special_tokens_matches_tiktoken&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;22 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_unicode_string_with_special_tokens_matches_tiktoken():\n        reference_tokenizer = tiktoken.get_encoding(&amp;quot;gpt2&amp;quot;)\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH, merges_path=MERGES_PATH, special_tokens=[&amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;]\n        )\n\ntests\\test_tokenizer.py:235: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B16DBC0B0&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_overlapping_special_tokens&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_overlapping_special_tokens&#34;, &#34;duration&#34;: &#34;115 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_overlapping_special_tokens&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;115 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_overlapping_special_tokens():\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH,\n            merges_path=MERGES_PATH,\n            special_tokens=[&amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;, &amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;lt;|endoftext|&amp;gt;&amp;quot;],\n        )\n\ntests\\test_tokenizer.py:249: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B18DA0410&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_address_roundtrip&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_address_roundtrip&#34;, &#34;duration&#34;: &#34;24 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_address_roundtrip&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;24 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_address_roundtrip():\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH,\n            merges_path=MERGES_PATH,\n        )\n\ntests\\test_tokenizer.py:266: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B16DD31D0&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_address_matches_tiktoken&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_address_matches_tiktoken&#34;, &#34;duration&#34;: &#34;25 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_address_matches_tiktoken&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;25 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_address_matches_tiktoken():\n        reference_tokenizer = tiktoken.get_encoding(&amp;quot;gpt2&amp;quot;)\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH,\n            merges_path=MERGES_PATH,\n        )\n\ntests\\test_tokenizer.py:279: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B1886F620&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_german_roundtrip&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_german_roundtrip&#34;, &#34;duration&#34;: &#34;24 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_german_roundtrip&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;24 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_german_roundtrip():\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH,\n            merges_path=MERGES_PATH,\n        )\n\ntests\\test_tokenizer.py:295: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B187885C0&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_german_matches_tiktoken&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_german_matches_tiktoken&#34;, &#34;duration&#34;: &#34;26 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_german_matches_tiktoken&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;26 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_german_matches_tiktoken():\n        reference_tokenizer = tiktoken.get_encoding(&amp;quot;gpt2&amp;quot;)\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH,\n            merges_path=MERGES_PATH,\n        )\n\ntests\\test_tokenizer.py:308: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B188BFE90&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_tinystories_sample_roundtrip&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_tinystories_sample_roundtrip&#34;, &#34;duration&#34;: &#34;24 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_tinystories_sample_roundtrip&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;24 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_tinystories_sample_roundtrip():\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH,\n            merges_path=MERGES_PATH,\n        )\n\ntests\\test_tokenizer.py:324: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B18740320&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_tinystories_matches_tiktoken&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_tinystories_matches_tiktoken&#34;, &#34;duration&#34;: &#34;26 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_tinystories_matches_tiktoken&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;26 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_tinystories_matches_tiktoken():\n        reference_tokenizer = tiktoken.get_encoding(&amp;quot;gpt2&amp;quot;)\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH, merges_path=MERGES_PATH, special_tokens=[&amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;]\n        )\n\ntests\\test_tokenizer.py:337: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B19A0C7A0&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_encode_special_token_trailing_newlines&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_encode_special_token_trailing_newlines&#34;, &#34;duration&#34;: &#34;26 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_encode_special_token_trailing_newlines&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;26 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_encode_special_token_trailing_newlines():\n        reference_tokenizer = tiktoken.get_encoding(&amp;quot;gpt2&amp;quot;)\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH, merges_path=MERGES_PATH, special_tokens=[&amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;]\n        )\n\ntests\\test_tokenizer.py:353: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B18864BC0&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_encode_special_token_double_newline_non_whitespace&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_encode_special_token_double_newline_non_whitespace&#34;, &#34;duration&#34;: &#34;25 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_encode_special_token_double_newline_non_whitespace&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;25 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_encode_special_token_double_newline_non_whitespace():\n        reference_tokenizer = tiktoken.get_encoding(&amp;quot;gpt2&amp;quot;)\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH, merges_path=MERGES_PATH, special_tokens=[&amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;]\n        )\n\ntests\\test_tokenizer.py:369: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B18E65010&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_encode_iterable_tinystories_sample_roundtrip&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_encode_iterable_tinystories_sample_roundtrip&#34;, &#34;duration&#34;: &#34;27 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_encode_iterable_tinystories_sample_roundtrip&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;27 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_encode_iterable_tinystories_sample_roundtrip():\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH,\n            merges_path=MERGES_PATH,\n        )\n\ntests\\test_tokenizer.py:384: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B1757ABD0&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_encode_iterable_tinystories_matches_tiktoken&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_encode_iterable_tinystories_matches_tiktoken&#34;, &#34;duration&#34;: &#34;22 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_encode_iterable_tinystories_matches_tiktoken&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;22 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_encode_iterable_tinystories_matches_tiktoken():\n        reference_tokenizer = tiktoken.get_encoding(&amp;quot;gpt2&amp;quot;)\n&amp;gt;       tokenizer = get_tokenizer_from_vocab_merges_path(\n            vocab_path=VOCAB_PATH, merges_path=MERGES_PATH, special_tokens=[&amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;]\n        )\n\ntests\\test_tokenizer.py:399: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\test_tokenizer.py:49: in get_tokenizer_from_vocab_merges_path\n    for line in f:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B18D52780&amp;gt;\ninput = b&amp;#x27;ial\\nott ed\\n\\xc4\\xa0V eg\\n\\xc4\\xa0Re uters\\n\\xc4\\xa0sw ord\\nps on\\n\\xc4\\xa0Rom ney\\na ed\\n\\xc4\\xa0t rib\\n\\xc4\\xa0in...L y\\n\\xc4\\xa0m ask\\n\\xc4\\xa0dis aster\\n\\xc4\\xa0ag es\\nER E\\nes is\\n\\xc4\\xa0H old\\n\\xc4\\xa0cas ual\\nb led\\n\\xc4\\xa0en a&amp;#x27;\nfinal = False\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1401: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_tokenizer.py::test_encode_iterable_memory_usage&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Skipped&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_encode_iterable_memory_usage::setup&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Skipped&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_encode_iterable_memory_usage::setup&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;(&amp;#x27;C:\\\\Users\\\\jrsta\\\\Documents\\\\GitHub\\\\CS336_Projects\\\\proj1_basic\\\\tests\\\\test_tokenizer.py&amp;#x27;, 416, &amp;#x27;Skipped: rlimit support for non-linux systems is spotty.&amp;#x27;)\n&#34;}], &#34;tests/test_tokenizer.py::test_encode_memory_usage&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Skipped&#34;, &#34;testId&#34;: &#34;tests/test_tokenizer.py::test_encode_memory_usage::setup&#34;, &#34;duration&#34;: &#34;0 ms&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Skipped&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_tokenizer.py::test_encode_memory_usage::setup&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;0 ms&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;(&amp;#x27;C:\\\\Users\\\\jrsta\\\\Documents\\\\GitHub\\\\CS336_Projects\\\\proj1_basic\\\\tests\\\\test_tokenizer.py&amp;#x27;, 431, &amp;#x27;Skipped: rlimit support for non-linux systems is spotty.&amp;#x27;)\n&#34;}], &#34;tests/test_train_bpe.py::test_train_bpe_speed&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_train_bpe.py::test_train_bpe_speed&#34;, &#34;duration&#34;: &#34;00:00:02&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_train_bpe.py::test_train_bpe_speed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:02&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_train_bpe_speed():\n        &amp;quot;&amp;quot;&amp;quot;\n        Ensure that BPE training is relatively efficient by measuring training\n        time on this small dataset and throwing an error if it takes more than 1.5 seconds.\n        This is a pretty generous upper-bound, it takes 0.38 seconds with the\n        reference implementation on my laptop. In contrast, the toy implementation\n        takes around 3 seconds.\n        &amp;quot;&amp;quot;&amp;quot;\n        input_path = FIXTURES_PATH / &amp;quot;corpus.en&amp;quot;\n        start_time = time.time()\n        _, _ = run_train_bpe(\n            input_path=input_path,\n            vocab_size=500,\n            special_tokens=[&amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;],\n        )\n        end_time = time.time()\n&amp;gt;       assert end_time - start_time &amp;lt; 1.5\nE       assert (1752061449.6167202 - 1752061447.172856) &amp;lt; 1.5\n\ntests\\test_train_bpe.py:24: AssertionError\n&#34;}], &#34;tests/test_train_bpe.py::test_train_bpe&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_train_bpe.py::test_train_bpe&#34;, &#34;duration&#34;: &#34;00:00:03&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_train_bpe.py::test_train_bpe&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:03&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;def test_train_bpe():\n        input_path = FIXTURES_PATH / &amp;quot;corpus.en&amp;quot;\n        vocab, merges = run_train_bpe(\n            input_path=input_path,\n            vocab_size=500,\n            special_tokens=[&amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;],\n        )\n    \n        # Path to the reference tokenizer vocab and merges\n        reference_vocab_path = FIXTURES_PATH / &amp;quot;train-bpe-reference-vocab.json&amp;quot;\n        reference_merges_path = FIXTURES_PATH / &amp;quot;train-bpe-reference-merges.txt&amp;quot;\n    \n        # Compare the learned merges to the expected output merges\n        gpt2_byte_decoder = {v: k for k, v in gpt2_bytes_to_unicode().items()}\n        with open(reference_merges_path,  encoding=&amp;quot;utf-8&amp;quot;) as f:\n            gpt2_reference_merges = [tuple(line.rstrip().split(&amp;quot; &amp;quot;)) for line in f]\n            reference_merges = [\n                (\n                    bytes([gpt2_byte_decoder[token] for token in merge_token_1]),\n                    bytes([gpt2_byte_decoder[token] for token in merge_token_2]),\n                )\n                for merge_token_1, merge_token_2 in gpt2_reference_merges\n            ]\n        assert merges == reference_merges\n    \n        # Compare the vocab to the expected output vocab\n        with open(reference_vocab_path) as f:\n&amp;gt;           gpt2_reference_vocab = json.load(f)\n                                   ^^^^^^^^^^^^\n\ntests\\test_train_bpe.py:54: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\..\\..\\anaconda3\\Lib\\json\\__init__.py:293: in load\n    return loads(fp.read(),\n                 ^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;encodings.cp1252.IncrementalDecoder object at 0x0000021B169C8F50&amp;gt;\ninput = b&amp;#x27;{\\n    &amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;: 0,\\n    &amp;quot;!&amp;quot;: 1,\\n    &amp;quot;\\\\&amp;quot;&amp;quot;: 2,\\n    &amp;quot;#&amp;quot;: 3,\\n    &amp;quot;$&amp;quot;: 4,\\n    &amp;quot;%&amp;quot;: 5,\\n    &amp;quot;&amp;amp;&amp;quot;: 6,\\n    &amp;quot;\\&amp;#x27;&amp;quot;...494,\\n    &amp;quot;\\xc4\\xa0work&amp;quot;: 495,\\n    &amp;quot;here&amp;quot;: 496,\\n    &amp;quot;fore&amp;quot;: 497,\\n    &amp;quot;\\xc4\\xa0sit&amp;quot;: 498,\\n    &amp;quot;\\xc4\\xa0ver&amp;quot;: 499\\n}&amp;#x27;\nfinal = True\n\n    def decode(self, input, final=False):\n&amp;gt;       return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       UnicodeDecodeError: &amp;#x27;charmap&amp;#x27; codec can&amp;#x27;t decode byte 0x81 in position 1707: character maps to &amp;lt;undefined&amp;gt;\n\n..\\..\\..\\..\\anaconda3\\Lib\\encodings\\cp1252.py:23: UnicodeDecodeError\n&#34;}], &#34;tests/test_train_bpe.py::test_train_bpe_special_tokens&#34;: [{&#34;extras&#34;: [], &#34;result&#34;: &#34;Failed&#34;, &#34;testId&#34;: &#34;tests/test_train_bpe.py::test_train_bpe_special_tokens&#34;, &#34;duration&#34;: &#34;00:00:15&#34;, &#34;resultsTableRow&#34;: [&#34;&lt;td class=\&#34;col-result\&#34;&gt;Failed&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-testId\&#34;&gt;tests/test_train_bpe.py::test_train_bpe_special_tokens&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-duration\&#34;&gt;00:00:15&lt;/td&gt;&#34;, &#34;&lt;td class=\&#34;col-links\&#34;&gt;&lt;/td&gt;&#34;], &#34;log&#34;: &#34;snapshot = &amp;lt;tests.conftest.Snapshot object at 0x0000021B169CA630&amp;gt;\n\n    def test_train_bpe_special_tokens(snapshot):\n        &amp;quot;&amp;quot;&amp;quot;\n        Ensure that the special tokens are added to the vocabulary and not\n        merged with other tokens.\n        &amp;quot;&amp;quot;&amp;quot;\n        input_path = FIXTURES_PATH / &amp;quot;tinystories_sample_5M.txt&amp;quot;\n        vocab, merges = run_train_bpe(\n            input_path=input_path,\n            vocab_size=1000,\n            special_tokens=[&amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;],\n        )\n    \n        # Check that the special token is not in the vocab\n        vocabs_without_specials = [word for word in vocab.values() if word != b&amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;]\n        for word_bytes in vocabs_without_specials:\n            assert b&amp;quot;&amp;lt;|&amp;quot; not in word_bytes\n    \n&amp;gt;       snapshot.assert_match(\n            {\n                &amp;quot;vocab_keys&amp;quot;: set(vocab.keys()),\n                &amp;quot;vocab_values&amp;quot;: set(vocab.values()),\n                &amp;quot;merges&amp;quot;: merges,\n            },\n        )\n\ntests\\test_train_bpe.py:82: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests\\conftest.py:146: in patched_assert_match\n    return original_assert_match(actual, test_name=test_name, force_update=force_update)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &amp;lt;tests.conftest.Snapshot object at 0x0000021B169CA630&amp;gt;\nactual = {&amp;#x27;merges&amp;#x27;: [(b&amp;#x27;h&amp;#x27;, b&amp;#x27;e&amp;#x27;), (b&amp;#x27; &amp;#x27;, b&amp;#x27;t&amp;#x27;), (b&amp;#x27; &amp;#x27;, b&amp;#x27;a&amp;#x27;), (b&amp;#x27; &amp;#x27;, b&amp;#x27;s&amp;#x27;), (b&amp;#x27; &amp;#x27;, b&amp;#x27;w&amp;#x27;), (b&amp;#x27;n&amp;#x27;, b&amp;#x27;d&amp;#x27;), ...], &amp;#x27;vocab_keys&amp;#x27;: {0, 1, 2, 3, 4, 5, ...}, &amp;#x27;vocab_values&amp;#x27;: {b&amp;#x27;\\x00&amp;#x27;, b&amp;#x27;\\x01&amp;#x27;, b&amp;#x27;\\x02&amp;#x27;, b&amp;#x27;\\x03&amp;#x27;, b&amp;#x27;\\x04&amp;#x27;, b&amp;#x27;\\x05&amp;#x27;, ...}}\ntest_name = &amp;#x27;test_train_bpe_special_tokens&amp;#x27;, force_update = False\n\n    def assert_match(\n        self,\n        actual: _A | dict[str, _A],\n        test_name: str,\n        force_update: bool = False,\n    ):\n        &amp;quot;&amp;quot;&amp;quot;\n        Assert that the actual data matches the snapshot.\n        Args:\n            actual: Single object or dictionary of named objects\n            test_name: The name of the test (used for the snapshot file)\n            force_update: If True, update the snapshot instead of comparing\n        &amp;quot;&amp;quot;&amp;quot;\n    \n        snapshot_path = self._get_snapshot_path(test_name)\n    \n    \n        # Load the snapshot\n        with open(snapshot_path, &amp;quot;rb&amp;quot;) as f:\n            expected_data = pickle.load(f)\n    \n        if isinstance(actual, dict):\n            for key in actual:\n                if key not in expected_data:\n                    raise AssertionError(f&amp;quot;Key &amp;#x27;{key}&amp;#x27; not found in snapshot for {test_name}&amp;quot;)\n&amp;gt;               assert actual[key] == expected_data[key], f&amp;quot;Data for key &amp;#x27;{key}&amp;#x27; does not match snapshot for {test_name}&amp;quot;\nE               AssertionError: Data for key &amp;#x27;vocab_values&amp;#x27; does not match snapshot for test_train_bpe_special_tokens\nE               assert {b&amp;#x27;\\x00&amp;#x27;, b&amp;#x27;\\... b&amp;#x27;\\x05&amp;#x27;, ...} == {b&amp;#x27;\\x00&amp;#x27;, b&amp;#x27;\\... b&amp;#x27;\\x05&amp;#x27;, ...}\nE                 \nE                 Extra items in the left set:\nE                 b&amp;#x27;oon&amp;#x27;\nE                 Extra items in the right set:\nE                 b&amp;#x27;\\n\\n&amp;#x27;\nE                 \nE                 Full diff:...\nE                 \nE                 ...Full output truncated (1003 lines hidden), use &amp;#x27;-vv&amp;#x27; to show\n\ntests\\conftest.py:119: AssertionError\n&#34;}]}, &#34;renderCollapsed&#34;: [&#34;passed&#34;], &#34;initialSort&#34;: &#34;result&#34;, &#34;title&#34;: &#34;report.html&#34;}"></div>
    <script>
      (function(){function r(e,n,t){function o(i,f){if(!n[i]){if(!e[i]){var c="function"==typeof require&&require;if(!f&&c)return c(i,!0);if(u)return u(i,!0);var a=new Error("Cannot find module '"+i+"'");throw a.code="MODULE_NOT_FOUND",a}var p=n[i]={exports:{}};e[i][0].call(p.exports,function(r){var n=e[i][1][r];return o(n||r)},p,p.exports,r,e,n,t)}return n[i].exports}for(var u="function"==typeof require&&require,i=0;i<t.length;i++)o(t[i]);return o}return r})()({1:[function(require,module,exports){
const { getCollapsedCategory, setCollapsedIds } = require('./storage.js')

class DataManager {
    setManager(data) {
        const collapsedCategories = [...getCollapsedCategory(data.renderCollapsed)]
        const collapsedIds = []
        const tests = Object.values(data.tests).flat().map((test, index) => {
            const collapsed = collapsedCategories.includes(test.result.toLowerCase())
            const id = `test_${index}`
            if (collapsed) {
                collapsedIds.push(id)
            }
            return {
                ...test,
                id,
                collapsed,
            }
        })
        const dataBlob = { ...data, tests }
        this.data = { ...dataBlob }
        this.renderData = { ...dataBlob }
        setCollapsedIds(collapsedIds)
    }

    get allData() {
        return { ...this.data }
    }

    resetRender() {
        this.renderData = { ...this.data }
    }

    setRender(data) {
        this.renderData.tests = [...data]
    }

    toggleCollapsedItem(id) {
        this.renderData.tests = this.renderData.tests.map((test) =>
            test.id === id ? { ...test, collapsed: !test.collapsed } : test,
        )
    }

    set allCollapsed(collapsed) {
        this.renderData = { ...this.renderData, tests: [...this.renderData.tests.map((test) => (
            { ...test, collapsed }
        ))] }
    }

    get testSubset() {
        return [...this.renderData.tests]
    }

    get environment() {
        return this.renderData.environment
    }

    get initialSort() {
        return this.data.initialSort
    }
}

module.exports = {
    manager: new DataManager(),
}

},{"./storage.js":8}],2:[function(require,module,exports){
const mediaViewer = require('./mediaviewer.js')
const templateEnvRow = document.getElementById('template_environment_row')
const templateResult = document.getElementById('template_results-table__tbody')

function htmlToElements(html) {
    const temp = document.createElement('template')
    temp.innerHTML = html
    return temp.content.childNodes
}

const find = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return elem.querySelector(selector)
}

const findAll = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return [...elem.querySelectorAll(selector)]
}

const dom = {
    getStaticRow: (key, value) => {
        const envRow = templateEnvRow.content.cloneNode(true)
        const isObj = typeof value === 'object' && value !== null
        const values = isObj ? Object.keys(value).map((k) => `${k}: ${value[k]}`) : null

        const valuesElement = htmlToElements(
            values ? `<ul>${values.map((val) => `<li>${val}</li>`).join('')}<ul>` : `<div>${value}</div>`)[0]
        const td = findAll('td', envRow)
        td[0].textContent = key
        td[1].appendChild(valuesElement)

        return envRow
    },
    getResultTBody: ({ testId, id, log, extras, resultsTableRow, tableHtml, result, collapsed }) => {
        const resultBody = templateResult.content.cloneNode(true)
        resultBody.querySelector('tbody').classList.add(result.toLowerCase())
        resultBody.querySelector('tbody').id = testId
        resultBody.querySelector('.collapsible').dataset.id = id

        resultsTableRow.forEach((html) => {
            const t = document.createElement('template')
            t.innerHTML = html
            resultBody.querySelector('.collapsible').appendChild(t.content)
        })

        if (log) {
            // Wrap lines starting with "E" with span.error to color those lines red
            const wrappedLog = log.replace(/^E.*$/gm, (match) => `<span class="error">${match}</span>`)
            resultBody.querySelector('.log').innerHTML = wrappedLog
        } else {
            resultBody.querySelector('.log').remove()
        }

        if (collapsed) {
            resultBody.querySelector('.collapsible > td')?.classList.add('collapsed')
            resultBody.querySelector('.extras-row').classList.add('hidden')
        } else {
            resultBody.querySelector('.collapsible > td')?.classList.remove('collapsed')
        }

        const media = []
        extras?.forEach(({ name, format_type, content }) => {
            if (['image', 'video'].includes(format_type)) {
                media.push({ path: content, name, format_type })
            }

            if (format_type === 'html') {
                resultBody.querySelector('.extraHTML').insertAdjacentHTML('beforeend', `<div>${content}</div>`)
            }
        })
        mediaViewer.setup(resultBody, media)

        // Add custom html from the pytest_html_results_table_html hook
        tableHtml?.forEach((item) => {
            resultBody.querySelector('td[class="extra"]').insertAdjacentHTML('beforeend', item)
        })

        return resultBody
    },
}

module.exports = {
    dom,
    htmlToElements,
    find,
    findAll,
}

},{"./mediaviewer.js":6}],3:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const storageModule = require('./storage.js')

const getFilteredSubSet = (filter) =>
    manager.allData.tests.filter(({ result }) => filter.includes(result.toLowerCase()))

const doInitFilter = () => {
    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)
}

const doFilter = (type, show) => {
    if (show) {
        storageModule.showCategory(type)
    } else {
        storageModule.hideCategory(type)
    }

    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)

    const sortColumn = storageModule.getSort()
    doSort(sortColumn, true)
}

module.exports = {
    doFilter,
    doInitFilter,
}

},{"./datamanager.js":1,"./sort.js":7,"./storage.js":8}],4:[function(require,module,exports){
const { redraw, bindEvents, renderStatic } = require('./main.js')
const { doInitFilter } = require('./filter.js')
const { doInitSort } = require('./sort.js')
const { manager } = require('./datamanager.js')
const data = JSON.parse(document.getElementById('data-container').dataset.jsonblob)

function init() {
    manager.setManager(data)
    doInitFilter()
    doInitSort()
    renderStatic()
    redraw()
    bindEvents()
}

init()

},{"./datamanager.js":1,"./filter.js":3,"./main.js":5,"./sort.js":7}],5:[function(require,module,exports){
const { dom, find, findAll } = require('./dom.js')
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const { doFilter } = require('./filter.js')
const {
    getVisible,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    getSortDirection,
    possibleFilters,
} = require('./storage.js')

const removeChildren = (node) => {
    while (node.firstChild) {
        node.removeChild(node.firstChild)
    }
}

const renderStatic = () => {
    const renderEnvironmentTable = () => {
        const environment = manager.environment
        const rows = Object.keys(environment).map((key) => dom.getStaticRow(key, environment[key]))
        const table = document.getElementById('environment')
        removeChildren(table)
        rows.forEach((row) => table.appendChild(row))
    }
    renderEnvironmentTable()
}

const addItemToggleListener = (elem) => {
    elem.addEventListener('click', ({ target }) => {
        const id = target.parentElement.dataset.id
        manager.toggleCollapsedItem(id)

        const collapsedIds = getCollapsedIds()
        if (collapsedIds.includes(id)) {
            const updated = collapsedIds.filter((item) => item !== id)
            setCollapsedIds(updated)
        } else {
            collapsedIds.push(id)
            setCollapsedIds(collapsedIds)
        }
        redraw()
    })
}

const renderContent = (tests) => {
    const sortAttr = getSort(manager.initialSort)
    const sortAsc = JSON.parse(getSortDirection())
    const rows = tests.map(dom.getResultTBody)
    const table = document.getElementById('results-table')
    const tableHeader = document.getElementById('results-table-head')

    const newTable = document.createElement('table')
    newTable.id = 'results-table'

    // remove all sorting classes and set the relevant
    findAll('.sortable', tableHeader).forEach((elem) => elem.classList.remove('asc', 'desc'))
    tableHeader.querySelector(`.sortable[data-column-type="${sortAttr}"]`)?.classList.add(sortAsc ? 'desc' : 'asc')
    newTable.appendChild(tableHeader)

    if (!rows.length) {
        const emptyTable = document.getElementById('template_results-table__body--empty').content.cloneNode(true)
        newTable.appendChild(emptyTable)
    } else {
        rows.forEach((row) => {
            if (!!row) {
                findAll('.collapsible td:not(.col-links', row).forEach(addItemToggleListener)
                find('.logexpander', row).addEventListener('click',
                    (evt) => evt.target.parentNode.classList.toggle('expanded'),
                )
                newTable.appendChild(row)
            }
        })
    }

    table.replaceWith(newTable)
}

const renderDerived = () => {
    const currentFilter = getVisible()
    possibleFilters.forEach((result) => {
        const input = document.querySelector(`input[data-test-result="${result}"]`)
        input.checked = currentFilter.includes(result)
    })
}

const bindEvents = () => {
    const filterColumn = (evt) => {
        const { target: element } = evt
        const { testResult } = element.dataset

        doFilter(testResult, element.checked)
        const collapsedIds = getCollapsedIds()
        const updated = manager.renderData.tests.map((test) => {
            return {
                ...test,
                collapsed: collapsedIds.includes(test.id),
            }
        })
        manager.setRender(updated)
        redraw()
    }

    const header = document.getElementById('environment-header')
    header.addEventListener('click', () => {
        const table = document.getElementById('environment')
        table.classList.toggle('hidden')
        header.classList.toggle('collapsed')
    })

    findAll('input[name="filter_checkbox"]').forEach((elem) => {
        elem.addEventListener('click', filterColumn)
    })

    findAll('.sortable').forEach((elem) => {
        elem.addEventListener('click', (evt) => {
            const { target: element } = evt
            const { columnType } = element.dataset
            doSort(columnType)
            redraw()
        })
    })

    document.getElementById('show_all_details').addEventListener('click', () => {
        manager.allCollapsed = false
        setCollapsedIds([])
        redraw()
    })
    document.getElementById('hide_all_details').addEventListener('click', () => {
        manager.allCollapsed = true
        const allIds = manager.renderData.tests.map((test) => test.id)
        setCollapsedIds(allIds)
        redraw()
    })
}

const redraw = () => {
    const { testSubset } = manager

    renderContent(testSubset)
    renderDerived()
}

module.exports = {
    redraw,
    bindEvents,
    renderStatic,
}

},{"./datamanager.js":1,"./dom.js":2,"./filter.js":3,"./sort.js":7,"./storage.js":8}],6:[function(require,module,exports){
class MediaViewer {
    constructor(assets) {
        this.assets = assets
        this.index = 0
    }

    nextActive() {
        this.index = this.index === this.assets.length - 1 ? 0 : this.index + 1
        return [this.activeFile, this.index]
    }

    prevActive() {
        this.index = this.index === 0 ? this.assets.length - 1 : this.index -1
        return [this.activeFile, this.index]
    }

    get currentIndex() {
        return this.index
    }

    get activeFile() {
        return this.assets[this.index]
    }
}


const setup = (resultBody, assets) => {
    if (!assets.length) {
        resultBody.querySelector('.media').classList.add('hidden')
        return
    }

    const mediaViewer = new MediaViewer(assets)
    const container = resultBody.querySelector('.media-container')
    const leftArrow = resultBody.querySelector('.media-container__nav--left')
    const rightArrow = resultBody.querySelector('.media-container__nav--right')
    const mediaName = resultBody.querySelector('.media__name')
    const counter = resultBody.querySelector('.media__counter')
    const imageEl = resultBody.querySelector('img')
    const sourceEl = resultBody.querySelector('source')
    const videoEl = resultBody.querySelector('video')

    const setImg = (media, index) => {
        if (media?.format_type === 'image') {
            imageEl.src = media.path

            imageEl.classList.remove('hidden')
            videoEl.classList.add('hidden')
        } else if (media?.format_type === 'video') {
            sourceEl.src = media.path

            videoEl.classList.remove('hidden')
            imageEl.classList.add('hidden')
        }

        mediaName.innerText = media?.name
        counter.innerText = `${index + 1} / ${assets.length}`
    }
    setImg(mediaViewer.activeFile, mediaViewer.currentIndex)

    const moveLeft = () => {
        const [media, index] = mediaViewer.prevActive()
        setImg(media, index)
    }
    const doRight = () => {
        const [media, index] = mediaViewer.nextActive()
        setImg(media, index)
    }
    const openImg = () => {
        window.open(mediaViewer.activeFile.path, '_blank')
    }
    if (assets.length === 1) {
        container.classList.add('media-container--fullscreen')
    } else {
        leftArrow.addEventListener('click', moveLeft)
        rightArrow.addEventListener('click', doRight)
    }
    imageEl.addEventListener('click', openImg)
}

module.exports = {
    setup,
}

},{}],7:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const storageModule = require('./storage.js')

const genericSort = (list, key, ascending, customOrder) => {
    let sorted
    if (customOrder) {
        sorted = list.sort((a, b) => {
            const aValue = a.result.toLowerCase()
            const bValue = b.result.toLowerCase()

            const aIndex = customOrder.findIndex((item) => item.toLowerCase() === aValue)
            const bIndex = customOrder.findIndex((item) => item.toLowerCase() === bValue)

            // Compare the indices to determine the sort order
            return aIndex - bIndex
        })
    } else {
        sorted = list.sort((a, b) => a[key] === b[key] ? 0 : a[key] > b[key] ? 1 : -1)
    }

    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const durationSort = (list, ascending) => {
    const parseDuration = (duration) => {
        if (duration.includes(':')) {
            // If it's in the format "HH:mm:ss"
            const [hours, minutes, seconds] = duration.split(':').map(Number)
            return (hours * 3600 + minutes * 60 + seconds) * 1000
        } else {
            // If it's in the format "nnn ms"
            return parseInt(duration)
        }
    }
    const sorted = list.sort((a, b) => parseDuration(a['duration']) - parseDuration(b['duration']))
    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const doInitSort = () => {
    const type = storageModule.getSort(manager.initialSort)
    const ascending = storageModule.getSortDirection()
    const list = manager.testSubset
    const initialOrder = ['Error', 'Failed', 'Rerun', 'XFailed', 'XPassed', 'Skipped', 'Passed']

    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    if (type?.toLowerCase() === 'original') {
        manager.setRender(list)
    } else {
        let sortedList
        switch (type) {
        case 'duration':
            sortedList = durationSort(list, ascending)
            break
        case 'result':
            sortedList = genericSort(list, type, ascending, initialOrder)
            break
        default:
            sortedList = genericSort(list, type, ascending)
            break
        }
        manager.setRender(sortedList)
    }
}

const doSort = (type, skipDirection) => {
    const newSortType = storageModule.getSort(manager.initialSort) !== type
    const currentAsc = storageModule.getSortDirection()
    let ascending
    if (skipDirection) {
        ascending = currentAsc
    } else {
        ascending = newSortType ? false : !currentAsc
    }
    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    const list = manager.testSubset
    const sortedList = type === 'duration' ? durationSort(list, ascending) : genericSort(list, type, ascending)
    manager.setRender(sortedList)
}

module.exports = {
    doInitSort,
    doSort,
}

},{"./datamanager.js":1,"./storage.js":8}],8:[function(require,module,exports){
const possibleFilters = [
    'passed',
    'skipped',
    'failed',
    'error',
    'xfailed',
    'xpassed',
    'rerun',
]

const getVisible = () => {
    const url = new URL(window.location.href)
    const settings = new URLSearchParams(url.search).get('visible')
    const lower = (item) => {
        const lowerItem = item.toLowerCase()
        if (possibleFilters.includes(lowerItem)) {
            return lowerItem
        }
        return null
    }
    return settings === null ?
        possibleFilters :
        [...new Set(settings?.split(',').map(lower).filter((item) => item))]
}

const hideCategory = (categoryToHide) => {
    const url = new URL(window.location.href)
    const visibleParams = new URLSearchParams(url.search).get('visible')
    const currentVisible = visibleParams ? visibleParams.split(',') : [...possibleFilters]
    const settings = [...new Set(currentVisible)].filter((f) => f !== categoryToHide).join(',')

    url.searchParams.set('visible', settings)
    window.history.pushState({}, null, unescape(url.href))
}

const showCategory = (categoryToShow) => {
    if (typeof window === 'undefined') {
        return
    }
    const url = new URL(window.location.href)
    const currentVisible = new URLSearchParams(url.search).get('visible')?.split(',').filter(Boolean) ||
        [...possibleFilters]
    const settings = [...new Set([categoryToShow, ...currentVisible])]
    const noFilter = possibleFilters.length === settings.length || !settings.length

    noFilter ? url.searchParams.delete('visible') : url.searchParams.set('visible', settings.join(','))
    window.history.pushState({}, null, unescape(url.href))
}

const getSort = (initialSort) => {
    const url = new URL(window.location.href)
    let sort = new URLSearchParams(url.search).get('sort')
    if (!sort) {
        sort = initialSort || 'result'
    }
    return sort
}

const setSort = (type) => {
    const url = new URL(window.location.href)
    url.searchParams.set('sort', type)
    window.history.pushState({}, null, unescape(url.href))
}

const getCollapsedCategory = (renderCollapsed) => {
    let categories
    if (typeof window !== 'undefined') {
        const url = new URL(window.location.href)
        const collapsedItems = new URLSearchParams(url.search).get('collapsed')
        switch (true) {
        case !renderCollapsed && collapsedItems === null:
            categories = ['passed']
            break
        case collapsedItems?.length === 0 || /^["']{2}$/.test(collapsedItems):
            categories = []
            break
        case /^all$/.test(collapsedItems) || collapsedItems === null && /^all$/.test(renderCollapsed):
            categories = [...possibleFilters]
            break
        default:
            categories = collapsedItems?.split(',').map((item) => item.toLowerCase()) || renderCollapsed
            break
        }
    } else {
        categories = []
    }
    return categories
}

const getSortDirection = () => JSON.parse(sessionStorage.getItem('sortAsc')) || false
const setSortDirection = (ascending) => sessionStorage.setItem('sortAsc', ascending)

const getCollapsedIds = () => JSON.parse(sessionStorage.getItem('collapsedIds')) || []
const setCollapsedIds = (list) => sessionStorage.setItem('collapsedIds', JSON.stringify(list))

module.exports = {
    getVisible,
    hideCategory,
    showCategory,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    setSort,
    getSortDirection,
    setSortDirection,
    getCollapsedCategory,
    possibleFilters,
}

},{}]},{},[4]);
    </script>
  </footer>
</html>